{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Nans and subgroups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one easy way to clean the dataset from its Nan values: deleting the rows where there is an outsider (value = -999.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n",
      "0.727544\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(tX != -999.0, axis=1)\n",
    "tX_clean = tX[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(tX.shape)\n",
    "print(tX_clean.shape)\n",
    "print(1-tX_clean.shape[0]/tX.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we see its bad cause we are reducing our dataset by 72.75%, which is really huge! So this is not the correct way to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus looked at some other information from our data, especially the information we get from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de label = 1:  85667 \n",
      "Nombre de label = -1:  164333\n",
      "Pourcentage de label 1:  0.342668 \n",
      "Pourcentage de label -1:  0.657332\n"
     ]
    }
   ],
   "source": [
    "count_1 = list(y).count(1)\n",
    "count_2 = list(y).count(-1)\n",
    "print(\"Nombre de label = 1: \", count_1 , \"\\nNombre de label = -1: \" , count_2)\n",
    "print(\"Pourcentage de label 1: \" , count_1/len(y), \"\\nPourcentage de label -1: \", count_2/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we also saw while analysing our dataset is that one particular feature is a categorical one: PRI_jet_num, which has 4 different values. By dividing our dataset in 4 subgroups, depending on the value of PRI_jet_num, and by analysing the Nan values in each subgroup, we see that indeed the samples in each subgroups have some specific columns that have Nans.\n",
    "\n",
    "This is why we decided to create 4 models instead of one.\n",
    "\n",
    "We will standardize each subgroup, and thus each subgroup will have a particular mean and std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0=tX[tX[:,dict['PRI_jet_num']]==0]\n",
    "tX_1=tX[tX[:,dict['PRI_jet_num']]==1]\n",
    "tX_2=tX[tX[:,dict['PRI_jet_num']]==2]\n",
    "tX_3=tX[tX[:,dict['PRI_jet_num']]==3]\n",
    "y_0=y[tX[:,dict['PRI_jet_num']]==0]\n",
    "y_1=y[tX[:,dict['PRI_jet_num']]==1]\n",
    "y_2=y[tX[:,dict['PRI_jet_num']]==2]\n",
    "y_3=y[tX[:,dict['PRI_jet_num']]==3]\n",
    "ids_0=ids[tX[:,dict['PRI_jet_num']]==0]\n",
    "ids_1=ids[tX[:,dict['PRI_jet_num']]==1]\n",
    "ids_2=ids[tX[:,dict['PRI_jet_num']]==2]\n",
    "ids_3=ids[tX[:,dict['PRI_jet_num']]==3]\n",
    "tX_list = [tX_0]\n",
    "tX_list.append(tX_1)\n",
    "tX_list.append(tX_2)\n",
    "tX_list.append(tX_3)\n",
    "ids_list = [ids_0]\n",
    "ids_list.append(ids_1)\n",
    "ids_list.append(ids_2)\n",
    "ids_list.append(ids_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to standardize each of our subgroups. Also, we will compute the indices of our tX dataset, where for the x_j_th datapoint, we have values not in the bounds of the percentile. Thus we store these j_th indexes, such we can remove them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplacÃ©s par la moyenne de la colonnes\n",
    "def replace_mean(tX_nan):\n",
    "    means_cols = np.nanmean(tX_nan,axis=1)\n",
    "    for row in range(0,tX_nan.shape[0]):\n",
    "        for col in range(0,tX_nan.shape[1]):\n",
    "            if np.isnan(tX_nan[row,col]):\n",
    "                tX_nan[row,col]=means_cols[col]\n",
    "    return (tX_nan)\n",
    "\n",
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.83469273209879\n",
      "63.38124719688545\n",
      "129.81575662840595\n",
      "139.61904053684216\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "std = []\n",
    "tX_nan_replaced = []\n",
    "for i in range(4):\n",
    "    x,m,s = standardize_NAN(tX_list[i])\n",
    "    tX_nan_replaced.append(replace_mean(x))\n",
    "    mean.append(m)\n",
    "    std.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, to send a correct file, we should group the dataset again as it was before divind it in subgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(ls,ids):\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    return data_ord[data_ord[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inorder = group(tX_nan_replaced, ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the boxplots of each features, but taking care that all Nan values per previously removes (tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(num, tX_clean):\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    data = []\n",
    "    for i in range(0, num):\n",
    "        data.append(tX_clean[:, i])\n",
    "    ax1.boxplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below is computing the indices of our tX dataset, where for the x_j_th datapoint, we have values not in the bounds of the percentile. Thus we store these j_th indexes, such we can remove them. This method computes this fot the i_th feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX_perc[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below thus removes all rows where the row has a value not in the percentile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_by_percentiles(tX,tX_clean,y):\n",
    "    args = []\n",
    "    for i in range(30):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.7)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    tX_perc = tX.copy()\n",
    "    y_perc = y.copy()\n",
    "    tX_perc = np.delete(tX_perc, mylist, axis=0)\n",
    "    y_perc = np.delete(y_perc,mylist,axis=0)\n",
    "    return tX_perc,y_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are plotting the correlation between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_of_2_features(y, tX, feat1, feat2):\n",
    "    y_copy = y.copy()\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    colormap = np.array(['r', 'g', 'b'])\n",
    "    #We change values -1 of y to 0 \n",
    "    y_copy[y_copy<0] = 0\n",
    "    categories = np.array(y_copy.copy())\n",
    "    categories = categories.astype(int)\n",
    "    plt.scatter(tX[:,feat1], tX[:,feat2],  c=colormap[categories])\n",
    "    plt.title(features[feat1] + ' and ' + features[feat2])\n",
    "    \n",
    "plot_of_2_features(y,tX,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can use a regression model to predict the missing value per feature, whithout taking into acount the outliers of a particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_par_feat(tX,y):\n",
    "    tX_model = tX.copy()\n",
    "    y_model = y.copy()\n",
    "    dic = {}\n",
    "    #finds all datapoints where at feature \"ind\" it has value -999.0 and stores it in a dictionnary\n",
    "    for ind, tX_ in enumerate(tX_model.T):\n",
    "        dic[ind] = list(np.where(tX_ == -999.0)[0])\n",
    "nan_par_feat(tX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_feature(tX,y):\n",
    "    tX_model = tX.copy()\n",
    "    y_model = y.copy()\n",
    "    dic = {}\n",
    "    #finds all datapoints where at feature \"ind\" it has value -999.0 and stores it in a dictionnary\n",
    "    for ind, tX_ in enumerate(tX_model.T):\n",
    "        dic[ind] = list(np.where(tX_ == -999.0)[0])\n",
    "    #We are gonna try to group features depending on the indices where they have -999.0\n",
    "    ls = {}\n",
    "    for ind in range(len(dic)):\n",
    "        for ind2 in range(ind+1,len(dic)):\n",
    "            sum_ = len(set(dic[ind]) & set(dic[ind2])) \n",
    "            if sum_ in ls:\n",
    "                if ind2 not in ls[sum_]:\n",
    "                    ls[sum_].append(ind2)\n",
    "            else:\n",
    "                ls[sum_] = [ind,ind2]\n",
    "    return ls\n",
    "dic = model_feature(tX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the k value of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_factors(x):\n",
    "    # This function takes a number and prints the factors\n",
    "    a = []\n",
    "    for i in range(2,10):\n",
    "        if x % i == 0:\n",
    "            a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_LS=tX_clean.copy()\n",
    "#y_LS= y_clean.copy()\n",
    "#accuracy=0.7239627683002026 mse=0.7401139564470155\n",
    "\n",
    "#tX_LS=tX_nan.copy()\n",
    "#y_LS= y.copy()\n",
    "# accuracy=0.745088 mse=0.6786542286532609\n",
    "\n",
    "def compute_least_squares(tX, y):\n",
    "    tX_LS=tX.copy()\n",
    "    y_LS= y.copy()\n",
    "\n",
    "    K_values = return_factors(len(tX_LS))\n",
    "    accuracy = []\n",
    "    #K-fold crossvalidation\n",
    "    for K in K_values:\n",
    "        #Initialization\n",
    "        list_tX_LS = np.split(tX_LS,K)\n",
    "        list_y_LS = np.split(y_LS,K)\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        opt_w = []\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train = np.concatenate(list_tX_LS[:ind] + list_tX_LS[ind+1:])\n",
    "            y_train = np.concatenate(list_y_LS[:ind] + list_y_LS[ind+1:])\n",
    "            mse_LS, optimal_weights_LS = least_squares(y_train,tX_train)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test, optimal_weights_LS))\n",
    "            weights.append(optimal_weights_LS)\n",
    "\n",
    "        opt_w = weights[np.argmin(mse_errors)]\n",
    "        y_model = predict_labels(opt_w, tX_LS)\n",
    "\n",
    "        #Computing accuracy\n",
    "        accuracy.append((list(y_model == y_LS).count(True))/len(y_model))\n",
    "        print(\"accuracy = {val} mse={mse}\".format(mse = mse_LS, val=accuracy[-1]))\n",
    "\n",
    "    #Plot of accuracies\n",
    "    print(\"\\nMaximum accuracy = {val}\".format(val=np.max(accuracy)))\n",
    "    #plt.plot(K_values, accuracy, '.-', markersize=15, label = \"Accuracy\");\n",
    "    #plt.xlabel(\"K value\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Accuracies for Least Squares\")\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "\n",
    "    list_tX_LR = np.split(tX_LR,K)\n",
    "    list_y_LR = np.split(y_LR,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LR):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LR[ind]\n",
    "            tX_train= list_tX_LR[:ind] + list_tX_LR[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LR[:ind] + list_y_LR[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "            loss, opt_w = logistic_regression(y_train,tX_train,w_initial, max_iters, gamma)\n",
    "            loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "\n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\" gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR, tX_LR)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LR == y_predicted).count(True))/len(y_LR))\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-eb89b51f335f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtx_perc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_perc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_rows_by_percentiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_nan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_clean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtx_perc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rows were deleted based on the percentile concept\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_nan' is not defined"
     ]
    }
   ],
   "source": [
    "tx_perc, y_perc = remove_rows_by_percentiles(tX_nan,tX_clean,y)\n",
    "print(tX.shape[0]-tx_perc.shape[0], \"rows were deleted based on the percentile concept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.751196 mse=0.676337897959684\n",
      "accuracy = 0.750892 mse=0.6772515063201519\n",
      "accuracy = 0.751328 mse=0.677095521350583\n",
      "accuracy = 0.751436 mse=0.6771358709269435\n",
      "\n",
      "Maximum accuracy = 0.751436\n"
     ]
    }
   ],
   "source": [
    "compute_least_squares(data_inorder, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All-NaN slice encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-0e41e711cfa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_LS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_inorder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_LS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcross_validation_LR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_LS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_LS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-179a52cf2390>\u001b[0m in \u001b[0;36mcross_validation_LR_demo\u001b[1;34m(x_LR, y_LR, K)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0moptimal_gamma_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanargmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0moptimal_weights_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_opt_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanargmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" gamma={l:.3f},loss={loss:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimal_gamma_LR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py\u001b[0m in \u001b[0;36mnanargmin\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All-NaN slice encountered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All-NaN slice encountered"
     ]
    }
   ],
   "source": [
    "#Data subgoup\n",
    "x_LS = data_inorder\n",
    "y_LS = y.copy()\n",
    "cross_validation_LR_demo(x_LS,y_LS,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './leastSquarePOLY' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(opt_w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
