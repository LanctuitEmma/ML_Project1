{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(tX != -999.0, axis=1)\n",
    "tX_clean = tX[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(tX.shape)\n",
    "print(tX_clean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "\n",
    "tX_nan, mean_x_nan, std_x_nan = standardize_NAN(tX)\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplac√©s par la moyenne de la colonnes\n",
    "means_cols = np.nanmean(tX_nan,axis=1)\n",
    "for row in range(0,tX_nan.shape[0]):\n",
    "    for col in range(0,tX_nan.shape[1]):\n",
    "        if np.isnan(tX_nan[row,col]):\n",
    "            tX_nan[row,col]=means_cols[col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Least squares gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can alter on gamma + w_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "tX_LS, m_X,s = standardize(tX_clean)\n",
    "tX_LS = tX_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "\n",
    "#With nan value replaced\n",
    "#tX_LS=tX_nan\n",
    "#y_LS= y\n",
    "\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-4,0,20)\n",
    "K=5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "gen_opt_w=[]\n",
    "gen_mse =[]\n",
    "\n",
    "#gamma selection\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "    #K-fold crossvalidation\n",
    "    for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "        tX_test = tX_bloc\n",
    "        y_test = list_y_LS[ind]\n",
    "        tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "        tX_train= np.concatenate(tX_train)\n",
    "        y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "        y_train=np.concatenate(y_train)\n",
    "        \n",
    "        mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "        mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "        weights.append(opt_w)\n",
    "    gen_mse.append(np.mean(mse_errors))\n",
    "    gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "optimal_gamma_LS_GD = gammas[np.argmin(gen_mse)]\n",
    "optimal_weights_LS_GD = gen_opt_w[np.argmin(gen_mse)]\n",
    "print(\" gamma={l:.3f},mse={mse:.3f}\".format(mse = np.min(gen_mse), l = optimal_gamma_LS_GD))\n",
    "\n",
    "#Training Accuracy\n",
    "y_model = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "sum_ = 0\n",
    "for i,v in enumerate(y_model):\n",
    "    if(v == y_LS[i]):\n",
    "        sum_ = sum_+1\n",
    "print(sum_/len(y_model))\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823, mse=\n",
    "#With tX_NAN : accuracy= 0.705, mse=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the gamma and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "\n",
    "#With nan value replaced\n",
    "tX_LS=tX_nan\n",
    "y_LS= y\n",
    "\n",
    "max_iters = 50\n",
    "max_batch_size = 5\n",
    "gammas = [0.001,0.01]\n",
    "batch_sizes = np.arange(max_batch_size)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "result_mse =[]\n",
    "result_opt_w=[]\n",
    "result_gamma=[]\n",
    "for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "    result_mse_gamma = []\n",
    "    result_opt_w_gamma = []\n",
    "    for ind_gamma,gamma in enumerate(gammas):\n",
    "        mse_errors=[]\n",
    "        weights=[]\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "    \n",
    "        result_mse_gamma.append(np.mean(mse_errors))\n",
    "        result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "    result_mse.append(np.min(result_mse_gamma))\n",
    "    result_gamma = np.argmin(result_mse_gamma)\n",
    "    result_opt_batch_w = weights[np.argmin(result_mse_gamma)]\n",
    "\n",
    "\n",
    "print(\" gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_mse), l =result_gamma[np.argmin(result_mse)], b=np.argmin(result_mse))\n",
    "\n",
    "optimal_weights_LS_SGD = result_opt_w[np.argmin(result_mse)]\n",
    "print(optimal_weights_LS_SDG)\n",
    "\n",
    "#Training Accuracy\n",
    "y_model = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "sum_ = 0\n",
    "for i,v in enumerate(y_model):\n",
    "    if(v == y_LS[i]):\n",
    "        sum_ = sum_+1\n",
    "print(sum_/len(y_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './leastSquareGD' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(optimal_weights_LS_GD, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
