{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project on Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "- We only use imported data without any modification\n",
    "- We then cleaned it by removing all samples containing at least one Nan value\n",
    "- We then partioned our dataset into 4 subgroups, depending on the value of feature PRI_jet_num. We then standardized each subgroup individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*\n",
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nan values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the Higgs dataset, we saw that there were a lot of missing values, corresponding to the -999.0 value. We thus decided to remove the samples that had at least one -999.0 outlier value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 72.7544 % of our training dataset.\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "print(\"We removed\", (1-x_clean.shape[0]/x.shape[0])*100, \"% of our training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that too many data samples are lost in x_clean. So it is not a good idea to remove these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values in dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we chose an alternative way to deal with the Nan values. We decided to replace each NaN value by the mean of the feature it is in. The mean was computed without taking into account the Nan values in the feature. This is a standardization concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_NAN(x):\n",
    "    x_nan = x.copy()\n",
    "    x_nan[x_nan==-999.0] = np.nan\n",
    "    return (standardize(x_nan))\n",
    "\n",
    "# All the Nan (corresponding to unknown values) were replaced by the mean value of the feature it is in.\n",
    "def replace_mean(x_nan):\n",
    "    means_cols = np.nanmean(x_nan, axis=1)\n",
    "    is_nan = np.isnan(x_nan)\n",
    "    for col in range(x_nan.shape[1]):\n",
    "        x_nan[is_nan[:, col], col] = means_cols[col]\n",
    "    return (x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the outliers of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assumed that the dataset can have some ouliers. So to deal with them we implemented some methods that can remove the datasamples where some percentile is trespassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionning of dataset, based on PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing some data analysis, we saw that a specific column was only composed of 4 discrete values. This column was the \"PRI_jet_num\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names and their respective indices\n",
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus partitionned our dataset in the 4 different subgroups, corresponding to the value of the PRI_jet_num. This way, when doing the standardization on the dataset, we didn't bias our samples: indeed, the 4 groups have different kind of means and standard deviations values, which we used when standardizing each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subgrouping\n",
    "def subgrouping(x,ids,dict):\n",
    "    x_0=x[x[:,dict['PRI_jet_num']]==0]\n",
    "    x_1=x[x[:,dict['PRI_jet_num']]==1]\n",
    "    x_2=x[x[:,dict['PRI_jet_num']]==2]\n",
    "    x_3=x[x[:,dict['PRI_jet_num']]==3]\n",
    "    x_0 = np.delete(x_0,dict['PRI_jet_num'],1)\n",
    "    x_1 = np.delete(x_1,dict['PRI_jet_num'],1)\n",
    "    x_2 = np.delete(x_2,dict['PRI_jet_num'],1)\n",
    "    x_3 = np.delete(x_3,dict['PRI_jet_num'],1)\n",
    "    x_list = [x_0, x_1, x_2, x_3]\n",
    "\n",
    "    ids_0=ids[x[:,dict['PRI_jet_num']]==0]\n",
    "    ids_1=ids[x[:,dict['PRI_jet_num']]==1]\n",
    "    ids_2=ids[x[:,dict['PRI_jet_num']]==2]\n",
    "    ids_3=ids[x[:,dict['PRI_jet_num']]==3]\n",
    "    ids_list = [ids_0]\n",
    "    ids_list.append(ids_1)\n",
    "    ids_list.append(ids_2)\n",
    "    ids_list.append(ids_3)\n",
    "    \n",
    "\n",
    "    #Standardization of subgroups\n",
    "    mean = []\n",
    "    std = []\n",
    "    x_nan_replaced = []\n",
    "    for i in range(4):\n",
    "        x_arr,m,s = standardize_NAN(x_list[i])\n",
    "        print(i, m, s)\n",
    "        x_nan_replaced.append(replace_mean(x_arr))\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return x_nan_replaced, ids_list\n",
    "    \n",
    "#Grouping them back again\n",
    "def group(l,ids,dict):\n",
    "    ls = l.copy()\n",
    "    for i in range(4):\n",
    "        ls[i] = np.insert(ls[i],dict['PRI_jet_num'],np.ones((len(ids),1))*i,axis=1)\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    x_new = data_ord[data_ord[:,0].argsort()]\n",
    "    x_new = x_new[:,1:]\n",
    "    \n",
    "    #print(f\"groups: tx {np.mean(x_new, axis=0)}{np.std(x_new,axis=0)}\")\n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonxh\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm[:,valid_columns] =  ( x_valid_cols - mean_x[None, :] ) / std_x[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1.20667654e+02  5.87862388e+01  8.18703092e+01  1.38238670e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.66496128e+00\n",
      "  1.38238669e+01  7.63770107e+01  1.39276344e+00 -9.10076857e-01\n",
      "  0.00000000e+00  3.40127231e+01 -2.48576361e-02 -1.56573619e-02\n",
      "  4.23642875e+01 -5.23114410e-02  4.23519862e-02  3.15367606e+01\n",
      " -2.44434558e-02  1.25860810e+02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00] [51.74971089 32.003391   38.04347886 16.67462352  0.          0.\n",
      "  0.          0.69329161 16.67462348 23.56093879  0.5815929   0.93670201\n",
      "  0.         15.2294643   1.23342423  1.81734217 14.58590597  1.31084863\n",
      "  1.81783473 20.29443907  1.81099743 53.0863344   0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n",
      "1 [ 1.22182109e+02  4.60536000e+01  8.22190327e+01  6.59030903e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.33968637e+00\n",
      "  1.66449998e+01  1.50368035e+02  1.44418464e+00  2.35611975e-01\n",
      "  0.00000000e+00  3.85893680e+01 -1.73527288e-03 -7.31197772e-03\n",
      "  4.67665026e+01  6.68554627e-03  4.48938280e-02  4.01233713e+01\n",
      " -5.57938719e-03  2.03324833e+02  6.50121630e+01 -8.00435882e-04\n",
      " -1.50522284e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.50121632e+01] [59.34450047 35.58096046 42.28784944 47.43070109  0.          0.\n",
      "  0.          0.73762801 17.00781447 65.18166668  0.84641974  1.1063327\n",
      "  0.         21.00556477  1.21818737  1.81444814 21.33541491  1.26220634\n",
      "  1.8154439  28.35714127  1.81192526 81.90061285 45.15421873  1.82158106\n",
      "  1.81751683  0.          0.          0.         45.15421875]\n",
      "2 [ 1.22653136e+02  3.83436116e+01  7.92133481e+01  1.02985028e+02\n",
      "  2.60653431e+00  3.91406450e+02 -1.11545755e+00  2.06060702e+00\n",
      "  1.72806027e+01  2.45776275e+02  1.45346067e+00  5.65612815e-01\n",
      "  4.92867504e-01  4.46522669e+01 -7.75858989e-04  6.93356359e-03\n",
      "  5.09439375e+01 -3.46678179e-03  4.68206197e-02  5.37573689e+01\n",
      "  3.34000278e-03  2.96011170e+02  9.84702076e+01 -2.44786518e-03\n",
      " -1.32897040e-02  5.17098657e+01 -1.36306794e-02  6.14402827e-03\n",
      "  1.50180076e+02] [5.57490423e+01 3.52735935e+01 3.99167316e+01 7.05285845e+01\n",
      " 1.81260349e+00 4.26848181e+02 3.82103428e+00 7.59346693e-01\n",
      " 2.04094791e+01 1.01355960e+02 1.03438579e+00 9.78237122e-01\n",
      " 3.98565482e-01 2.79837976e+01 1.17217880e+00 1.81582136e+00\n",
      " 2.68817529e+01 1.19361204e+00 1.81531403e+00 4.02302623e+01\n",
      " 1.81482674e+00 1.11482689e+02 6.18663652e+01 1.81413389e+00\n",
      " 1.80990025e+00 2.54598156e+01 2.13480147e+00 1.81784901e+00\n",
      " 7.76454786e+01]\n",
      "3 [ 1.23189990e+02  4.21202026e+01  7.89255219e+01  1.26066343e+02\n",
      "  1.94277017e+00  3.27179877e+02 -1.53947347e-01  1.88462029e+00\n",
      "  5.35485356e+01  3.58008310e+02  1.58073610e+00  5.45345109e-01\n",
      "  3.79694324e-01  4.67709378e+01 -3.88079769e-03 -1.17623173e-02\n",
      "  5.59169127e+01  2.68678939e-04  3.67354268e-02  6.58187639e+01\n",
      "  7.97703483e-03  4.14852725e+02  1.23107808e+02 -1.38099170e-02\n",
      " -1.04990074e-03  7.12484564e+01 -7.78699693e-03 -1.91442880e-02\n",
      "  2.55320458e+02] [ 70.89775021  38.07417941  48.78977838  83.81996789   1.46989267\n",
      " 317.27268684   2.86726977   0.8272412   32.29848326 150.13499581\n",
      "   1.24713314   0.94333732   0.38762705  32.43010842   1.20470959\n",
      "   1.8241134   32.96582687   1.21765619   1.81806005  49.69728866\n",
      "   1.81234158 158.82544995  76.31203429   1.57322807   1.80673322\n",
      "  40.12542351   1.77531376   1.8147406  128.45133644]\n"
     ]
    }
   ],
   "source": [
    "x_subgroups_list, ids_list = subgrouping(x,ids,dict)\n",
    "x_subgroups = group(x_subgroups_list, ids_list,dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying all features that had Nan Values, we saw that many sample of our dataset had to replace their Nan value ba the mean value of the feature, as explained above in the standardization part. The problem is that having a mean value for 75% of our datasample is bad for some features. That's why we decided to remove some of them, and play only with the features that are essential to our model.\n",
    "\n",
    "Indeed, the columns that were removed were all features that derived from some primitive ones. And as most of them had many Nan values, we assumed it was a good idea to remove them and see how our model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    x_selected = np.zeros((len(x), len(selected_columns0)))\n",
    "    for i in range(len(x)):\n",
    "        s = np.take(x[i], indices=selected_columns0, axis=0)\n",
    "        x_selected[i] = s\n",
    "    return x_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = selected_non_nan_columns(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build polynomial features, meaning we expanded the number of features we had by adding features with an incremeneted degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    nb_features = x.shape[1]\n",
    "    nb_samples = x.shape[0]\n",
    "    x_poly = np.ones((nb_samples, 1))\n",
    "    for d in range(1, degree+1):\n",
    "        x_d = x**d\n",
    "        x_poly = np.hstack((x_poly, x_d))\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_s, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of the different ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least Squares Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\"   gamma={l:.3f}, mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least Squares Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 50\n",
    "    max_batch_size = 32\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "        result_mse_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LS[ind]\n",
    "                tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "                mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_mse_gamma.append(np.mean(mse_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_mse.append(np.min(result_mse_gamma))\n",
    "        result_gamma.append(gammas[np.argmin(result_mse_gamma)])\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_mse_gamma)])\n",
    "\n",
    "    print(\"   gamma={l:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = np.nanmin(result_mse), l =result_gamma[np.nanargmin(result_mse)], b=np.nanargmin(result_mse)))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "    \n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_squares(x, y):\n",
    "    x_LS= x.copy()\n",
    "    y_LS= y.copy()\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "    opt_w = []\n",
    "    K_values = return_factors(len(x_LS))\n",
    "    #K-fold crossvalidation\n",
    "    for K in K_values:\n",
    "        #Initialization\n",
    "        list_x_LS = np.split(x_LS,K)\n",
    "        list_y_LS = np.split(y_LS,K)\n",
    "        for ind, x_bloc in enumerate(list_x_LS):\n",
    "            x_test = x_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            x_train = np.concatenate(list_x_LS[:ind] + list_x_LS[ind+1:])\n",
    "            y_train = np.concatenate(list_y_LS[:ind] + list_y_LS[ind+1:])\n",
    "            mse_LS, optimal_weights_LS = least_squares(y_train,x_train)\n",
    "            mse_errors.append(compute_mse(y_test, x_test, optimal_weights_LS))\n",
    "            weights.append(optimal_weights_LS)\n",
    "\n",
    "    opt_w = weights[np.argmin(mse_errors)]\n",
    "    y_model = predict_labels(opt_w, x_LS)\n",
    "\n",
    "    #Computing accuracy\n",
    "    print(\"   mse={mse}\".format(mse = mse_LS))\n",
    "    accuracy = (list(y_model == y_LS).count(True))/len(y_model)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo_RR(x,y,K=4):\n",
    "    seed = 1\n",
    "    degree = 4\n",
    "    k_fold = K\n",
    "    lambdas = np.logspace(-4, 0, 20)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    x_test = x[k_indices[0]]\n",
    "    x_train = np.delete(x, [k_indices[0]], axis=0)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for i in range(len(lambdas)):\n",
    "        l = lambdas[i]\n",
    "        avg_err_tr = 0\n",
    "        avg_err_te = 0\n",
    "        for k in range(k_fold):\n",
    "            err = cross_validation_rr(y, x, k_indices, k, l, degree)\n",
    "            avg_err_tr += err[0]\n",
    "            avg_err_te += err[1]\n",
    "        rmse_tr.append(np.sqrt(2 * avg_err_tr / k_fold))\n",
    "        rmse_te.append(np.sqrt(2 * avg_err_te / k_fold))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    min_err_index = 0\n",
    "    for i in range(1, len(rmse_te)):\n",
    "        if rmse_te[i] < rmse_te[min_err_index]:\n",
    "            min_err_index = i\n",
    "            \n",
    "    print('Best lambda is: {0}'.format(lambdas[min_err_index]))\n",
    "    print(\"Loss:\")\n",
    "    return lambdas[min_err_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x_s,x,y,degree_opt=4):\n",
    "    x_poly = build_poly(x_s, degree_opt)\n",
    "    lambda_opt = cross_validation_demo_RR(x,y,K)\n",
    "    w_rr_opt, loss_tr = ridge_regression_s(y_s, x_poly, lambda_opt)\n",
    "    print(\"Training set mse: {}\".format(loss_tr))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_clean == y_predicted).count(True))/len(y_clean))\n",
    "    print(\"accuracy = {val}\".format(val=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "\n",
    "    list_tX_LR = np.split(tX_LR,K)\n",
    "    list_y_LR = np.split(y_LR,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LR):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LR[ind]\n",
    "            tX_train= list_tX_LR[:ind] + list_tX_LR[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LR[:ind] + list_y_LR[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "            loss, opt_w = logistic_regression(y_train,tX_train,w_initial, max_iters, gamma)\n",
    "            loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        \n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\"   gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR, tX_LR)\n",
    "    accuracy = (list(y_predicted == y_LR).count(True))/len(y_LR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "\n",
    "    max_iters = 50\n",
    "    #lambdas = np.logspace(-4,0,10)\n",
    "    lambdas = [0.006]\n",
    "    #gammas = np.logspace(-4,0,20)\n",
    "    gammas = [1e-4]\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    list_tX_LRR = np.split(tX_LRR,K)\n",
    "    list_y_LRR = np.split(y_LRR,K)\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind,lambda_ in enumerate(lambdas):  \n",
    "        result_loss_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LRR):\n",
    "                \n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LRR[ind]\n",
    "                tX_train= list_tX_LRR[:ind] + list_tX_LRR[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LRR[:ind] + list_y_LRR[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                loss, opt_w = reg_logistic_regression(y_train,tX_train,lambda_,w_initial,max_iters,gamma)\n",
    "                loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_loss_gamma.append(np.mean(loss_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_loss.append(np.min(result_loss_gamma))\n",
    "        result_gamma.append(np.argmin(result_loss_gamma))\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_loss_gamma)])\n",
    "\n",
    "    del result_loss_gamma\n",
    "    del result_opt_w_gamma\n",
    "    del loss_errors\n",
    "    del weights\n",
    "    print(np.min(result_loss))\n",
    "    print(result_gamma[np.argmin(result_loss)])\n",
    "    print(np.argmin(result_loss))\n",
    "    print(\"   gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_loss), l =result_gamma[np.argmin(result_loss)], b=np.argmin(result_loss)))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "    print(optimal_weights_LRR)\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR, tX_LRR)\n",
    "    accuracy = (list(y_predicted == y_LRR).count(True))/len(y_LRR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for cleaned dataset (with removed samples containing Nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n",
      "Least-Square-SDG\n",
      "Least-Square\n",
      "   mse=0.739442671577914\n",
      "   accuracy=0.720\n",
      "Ridge Regression\n",
      "Best lambda is: 0.007847599703514606\n",
      "Logistic Regression\n",
      "   gamma=0.234,loss=5875.691\n",
      "   accuracy=0.726\n",
      "Regularized Logistic Regression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lambdas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3d37d754e82c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcross_validation_LR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularized Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcross_validation_LRR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-e12241851d89>\u001b[0m in \u001b[0;36mcross_validation_LRR_demo\u001b[1;34m(x_LRR, y_LRR, K)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mresult_opt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mresult_gamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mresult_loss_gamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mresult_opt_w_gamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambdas' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VNX5+PHPkxD2RVmMICJQrYoaVBBFAYMVFEHUalW+LnUr9dtvFRekolZxX6hLqQvFlh8tImjBrYiKIBEpAQEF2RSVRVJQNiEEIWR5fn+cGTIJyeTOZO7MJPO8X695Te46J4dwnznn3PscUVWMMcaY6qQlugDGGGNqBwsYxhhjPLGAYYwxxhMLGMYYYzyxgGGMMcYTCxjGGGM8sYBhTAKIyHoROTfw8z0i8jcv+0bxOb1F5Ktoy2lMqHqJLoAxqU5VH4vVuUREgWNU9ZvAuT8Bjo3V+U1qsxaGqZNExL4MGRNjFjBMrSIiR4rIGyKyVUS2i8jzgfXXich/RORZEdkBjBKRNBG5T0Q2iMgWEfmniLQI7N9QRF4JnGOniCwSkcyQc60Vkd0isk5ErqqkHO1EZK+ItAxZd4qIbBORDBH5mYh8FDj/NhGZJCKHVPE7jRKRV0KWrwmUebuI3Fth3x4ikhso82YReV5E6ge2zQ3stkxECkTkChHJFpG8kOOPF5GcwPErRWRwyLYJIvKCiLwb+N0XisjPIv9XMnWVBQxTa4hIOjAd2AB0BI4ApoTscjqwFjgMeBS4LvDqC3QGmgLPB/b9NdACOBJoBdwM7BWRJsAYYICqNgPOBJZWLIuqbgJygUtDVv8PMFVViwABHgfaAccHPmeUh9+xC/AScE3g2FZA+5BdSoDbgdZAT+AXwO8CZeoT2KerqjZV1dcqnDsD+DcwM1BHtwCTRCS0y2oI8CBwKPANrh6NASxgmNqlB+4iepeq7lHVfao6L2T7JlX9i6oWq+pe4CrgGVVdq6oFwEjgykB3VRHuYny0qpao6hJVzQ+cpxQ4UUQaqepmVV1ZRXlexV1gEREBrgysQ1W/UdUPVbVQVbcCzwBne/gdLwOmq+pcVS0E/hgoD4HzLlHVBYHfcT3wV4/nBTgDFzSfUNX9qvoRLgAPCdnnDVX9VFWLgUnAyR7PbVKABQxTmxwJbAhczCqzscJyO1xrJGgD7kaPTGAi8AEwRUQ2ichTIpKhqnuAK3Atjs2B7pnjqvi8qUBPEWkH9AEU+ARARA4TkSki8l8RyQdewbUKqtMu9PcIlGd7cFlEfi4i00Xk+8B5H/N43gPnVtXSkHUbcC21oO9Dfv4JF2CMASxgmNplI9AhzIB2xdTLm4CjQpY7AMXAD6papKoPqmoXXLfTIOBaAFX9QFX7AW2BL4GXK/0w1Z247p3Lcd1Rk7Us/fPjgfJkqWpz4GpcN1V1NuMCIwAi0hjXEgp6KVCmYwLnvcfjecHVx5EiEvr/vgPwX4/HmxRnAcPUJp/iLqhPiEiTwMD1WWH2nwzcLiKdRKQp7tv4a6paLCJ9ReSkwLhIPq6LqkREMkVkcGAsoxAowI0bVOVVXKC5NPBzULPAsTtF5AjgLo+/41RgkIj0CgxmP0T5/6fNAuUtCLR8/rfC8T/gxmsqsxDYA4wIDMxnAxdSfhzImCpZwDC1hqqW4C5wRwPfAXm47qOqjMd1Pc0F1gH7cAO9AIfjLs75wGrgY1y3URpwJ+7b+A7c+MDvwnzGO8AxuFbLspD1DwKnAruAd4E3PP6OK4H/wwWfzcCPgd8zaDiuNbMb1/J5rcIpRgH/CNwFdXmFc+8HBgMDgG3Ai8C1qvqll7IZIzaBkjHGGC+shWGMMcYTCxjGGGM8sYBhjDHGEwsYxhhjPLGAYYwxxpM6ldGzdevW2rFjx6iO3bNnD02aNIltgeowq6/IWH1FxuorMjWpryVLlmxT1TZe9q1TAaNjx44sXrw4qmNzcnLIzs6ObYHqMKuvyFh9RcbqKzI1qS8R2VD9Xo51SRljjPHEt4AhIuMDcxCsqGL7VSLyReA1X0S6VtieLiKfi8h0v8pojDHGOz9bGBOA88NsXwecrapZwMPAuArbh+FSNhhjjEkCvo1hqOpcEekYZvv8kMUFhEwSIyLtgYG4yVvuqEk5ioqKyMvLY9++fWH3a9GiBatXW3zyqmnTphQVFZGRkZHoohhj4iRZBr1vBN4LWX4OGIHLzFkjeXl5NGvWjI4dO+LmuKnc7t27adasxh+XElSVvLw88vLy6NSpU6KLY4yJk4QHDBHpiwsYvQLLg4AtqrokkH65uuOHAkMBMjMzycnJKbe9RYsWtGrVioKCgrDnKSkpYffu3dH8CimpadOmrF+//qD6NpUrKCiwuoqA1Zd3zVeuJPPTT/ls5UryTzjB18/yNVttoEtquqqeWMX2LOBN3PzJawLrHsfNZ1wMNASa46aNvLq6z+vevbtWvK129erVHH/88dWW1VoYkdm9ezd5eXme6tbYbaKRsvryKDcX+vZF9+9HGjaE2bOhZ8+ITiEiS1S1u5d9E3ZbrYh0wM0RcE0wWACo6khVba+qHXFzJH/kJVgkq507d/Liiy9GdewFF1zAzp07Y1wiY0ydkZMDhYWIKuzf75Z95OdttZOBXOBYEckTkRtF5GYRuTmwy/24qSdfFJGlIhLdE3dJLlzAKCkJN5EbzJgxg0MOOSSm5SkuLg67XJXqymqMSYBAa0IB6tcHn1tlft4lNaSa7TcBN1WzTw6QE7tSeZOb6wJ1dnbErbuD3H333Xz77becfPLJ9OvXj4EDB/Lggw/Stm1bli5dyqpVq7j44ovZuHEj+/btY9iwYQwdOhQoe3K9oKCAAQMG0KtXL+bPn88RRxzB22+/TaNGjcp91tatW7n55pv57rvvAHjuuec466yzGDVqFJs2bWL9+vW0bt2a/v378+6777Jv3z727NnD7NmzGTFiBO+99x4iwn333ccVV1xBTk7OQWU1xiSRXbsA+P7882l7//01v2BVI+GD3vF0222wdGnl20pKGpGe7ur/iy+gtBTS0iArC1q0qPqcJ58Mzz1X9fYnnniCFStWsDTwwTk5OXz66aesWLHiwB1G48ePp2XLluzdu5fTTjuNSy+9lFatWpU7z9dff83kyZN5+eWXufzyy5k2bRpXX12+p27YsGHcfvvt9OrVi++++47zzjvvwK3CS5YsYd68eTRq1IgJEyaQm5vLF198QcuWLZk2bRpLly5l2bJlbNu2jdNOO40+ffoAHFRWY0wSmTYNDj2UNXfeSVufgwWkWMDwYtcuFyzAve/aFT5gRKNHjx7lLsBjxozhzTffBGDjxo18/fXXBwWMTp06cfLJJwPQrVs31q9ff9B5Z82aVa4VkJ+ff+DOr8GDB5drkfTr14+WLVsCMG/ePIYMGUJ6ejqZmZmcffbZLFq0iObNmx9UVmNMkigshHfegV/+Eq0Xn0t5SgWMcC2B3bv30qxZM3Jz4Re/cONH9evDpEmxb+WFZpXMyclh1qxZ5Obm0rhxY7Kzsyt9yLBBgwYHfk5PT2fv3r0H7VNaWkpubu5BXVUVP7Picrg75SxjqDFJavZs94320kvj9pGWfLCCnj3dv8PDD0d1h9pBmjVrFvb5jl27dnHooYfSuHFjvvzySxYsWBD1Z/Xv35/nn3/+wPLSqvrfKujTpw+vvfYaJSUlbN26lblz59KjR4+oy2GMiYNp06B5czj33Lh9pAWMSvTsCSNHxqZl0apVK8466yxOPPFE7rrrroO2n3/++RQXF5OVlcUf//hHzjjjjKg/a8yYMSxevJisrCy6dOnC2LFjPR13ySWXkJWVRdeuXTnnnHN46qmnOPzww6MuhzHGZ0VF8NZbcOGFENL74DdfH9yLN3twL37swb3I2INokbH6qsaHH0L//vDmm3DxxTWdDyP5H9wzxhgTpWnToEkTOO+8uH6sBQxjjKlNSkpcy2LgQKjkBhc/WcAwxpjaZN482LIFLrss7h9tAcMYY2qTqVOhYUMYMCDuH20BwxhjaovSUjd+MWAANG0a94+3gGGMMbXFggWweXNCuqPAAobvapLeHFwCwZ9++imGJTLG1FpTp7oUFIMGJeTjLWD4LNEBI9p05l73M8bEiarrjurf3z3hnQAplUvKsxjmN6+Y3nz06NGMHj2a119/ncLCQi655BIefPBB9uzZw+WXX05eXh4lJSX88Y9/5IcffmDTpk307duX1q1bM2fOnHLnXrJkCXfccQcFBQW0bt2aCRMm0LZtW7KzsznzzDP5z3/+w+DBg1m+fDktW7bk888/59RTT+Xee+/lhhtuYO3atTRu3Jhx48aRlZV1UBr0V199tUa/uzEmhhYvhu++g4ceSlgRUitghMlv3qikBD/ym1dMbz5z5ky+/vprPv30U1SVwYMHM3fuXLZu3Uq7du149913AZdjqkWLFjzzzDPMmTOH1q1blztvUVERt9xyC2+//TZt2rThtdde495772X8+PGAa9l8/PHHAFx33XWsWbOGWbNmkZ6ezi233MIpp5zCW2+9xUcffcS11157oHyhadCNMUlk6lSoVw8GD05YEVIrYHjhc37zmTNnMnPmTE455RTATXb/9ddf07t3b4YPH84f/vAHBg0aRO/evcOe56uvvmLFihX069cPcDPitW3b9sD2K664otz+v/rVr0hPTwdcOvNp06YBcM4557B9+3Z2BSZiqZgG3RiTBILdUb/4BRx6aMKKkVoBI0xLYG8wl5TP+c1VlZEjR/Lb3/72oG1LlixhxowZjBw5kv79+3P//feHPc8JJ5xAbm5updsjTWcuIpUeZ4xJAsuWwbffwh/+kNBi2KB3RTHOb14xvfl5553H+PHjKSgoAOC///0vW7ZsYdOmTTRu3Jirr76a4cOH89lnn1V6fNCxxx7L1q1bDwSMoqIiVq5c6alMffr0YdKkSYBL8ta6dWuaJ2gQzRjjwbRprov84osTWozUamF41bNnzFoVoenNBwwYwOjRo1m9ejU9A+dv2rQpr7zyCt988w133XUXaWlpZGRk8NJLLwEwdOhQBgwYQNu2bcsNetevX5+pU6dy6623smvXLoqLi7nttts44YQTqi3TqFGjuP7668nKyqJx48b84x//iMnvaozxgSr861/uJpw2bRJaFEtvHmDpzSNj6c0jY+m6I2P1FWLlSjjxRHjhBfjd7yrdpdanNxeR8SKyRURWVLH9KhH5IvCaLyJdA+uPFJE5IrJaRFaKyDC/ymiMMUlv2jQQgUsuSXRJfB3DmACcH2b7OuBsVc0CHgbGBdYXA3eq6vHAGcD/iUgXH8tpjDHJa+pU6NULQu6CTBTfAoaqzgV2hNk+X1V/DCwuANoH1m9W1c8CP+8GVgNH+FVOY4xJWmvWwPLlcOmliS4JkDyD3jcC71VcKSIdgVOAhVUdKCJDgaEAmZmZ5OTklNveokUL8vPzD9w2WpWSkpJK70YylSsuLmbfvn0H1bepXEFBgdVVBKy+nA6TJtEZyG3blsIw9RGv+vJ10DtwwZ+uqieG2acv8CLQS1W3h6xvCnwMPKqqb3j5vMoGvdetW0ezZs1o1apV2KBhg97eqSp5eXkUFxfTqVOnRBenVrBB3MhYfQV06wYZGS5LbRjxGvROaAtDRLKAvwEDKgSLDGAaMMlrsKhK+/btycvLY+vWrWH327dvHw0bNqzJR6WUPXv20LVr10QXw5i6a906+OwzGD060SU5IGEBQ0Q6AG8A16jqmpD1AvwdWK2qz9T0czIyMjx9C87JyTmQrsNULycnh4yMjEQXw5i6K5C+J1nGL8DHgCEik4FsoLWI5AEPABkAqjoWuB9oBbwY6CoqDjSLzgKuAZaLSDBT4D2qOsOvshpjTNKZOhVOPRWSqNvXt4ChqkOq2X4TcFMl6+cB4UeojTGmLtu4ERYuhMceS3RJyrFcUsYYk2zeCAzdJlF3FFjAMMaY5DNtGpx0Evz854kuSTkWMIwxJpls3gzz5sFllyW6JAexgGGMMcnkzTddhtok644CCxjGGJNcpk2D446DLsmXQs8ChjHGJIutWyEnx3VHVZPOKBEsYBhjTLJ4+20oLU3K7iiwgGGMMclj6lT42c8gSdPuWMAwxphk8OOPMHu2a10kYXcUWMAwxpjk8M47UFyclLfTBlnAMMaYZDB1KnToAN09ZRpPCAsYxhiTaPn5MHNmUndHgQUMY4xJvOnTYf/+pO6OAgsYxhiTeFOnQrt2cMYZiS5JWBYwjDEmkWbPhn//G3r2hLTkviQnd+mMMaYuy82FCy5wd0dNn+6Wk5gFDGOMSZScHDd2AS5o5OQksjTVsoBhjDGJ0q2bexeB+vUhOzuhxamOBQxjjEmUDRvc+//+rxvL6NkzseWphm9zehtjjKnGxIkulfnzzyf18xdB1sIwxphEWL8ePvkErrmmVgQL8DFgiMh4EdkiIiuq2H6ViHwReM0Xka4h284Xka9E5BsRuduvMhpjTMK88op7v+qqxJYjAn62MCYA54fZvg44W1WzgIeBcQAikg68AAwAugBDRCT5pp4yxphoqbruqLPPhqOOSnRpPPMtYKjqXGBHmO3zVfXHwOICoH3g5x7AN6q6VlX3A1OAi/wqpzHGxN2iRbBmjeuOqkWSZdD7RuC9wM9HABtDtuUBp1d1oIgMBYYCZGZmkhPlfcwFBQVRH5uKrL4iY/UVmbpeX0ePGUPb+vWZn5lJSQx+z3jVV8IDhoj0xQWMXsFVleymVR2vquMIdGd1795ds6O8jzknJ4doj01FVl+RsfqKTJ2ur6Ii+NWv4OKL6T1oUExOGa/6SmjAEJEs4G/AAFXdHlidBxwZslt7YFO8y2aMMb54/33Ytq3WdUdBAm+rFZEOwBvANaq6JmTTIuAYEekkIvWBK4F3ElFGY4yJuYkToU0bOO+8RJckYr61MERkMpANtBaRPOABIANAVccC9wOtgBfF3YNcrKrdVbVYRH4PfACkA+NVdaVf5TTGmLjZudNNxTp0KGRkJLo0EfMtYKjqkGq23wTcVMW2GcAMP8pljDEJ869/QWFhreyOAnvS2xhj4mfiRDj22KSetzscCxjGGBMPtTAVSEUWMIwxJh5qYSqQiixgGGOM34KpQPr0gY4dE12aqFnAMMYYv9XSVCAVWcAwxhi/TZwIDRrAZZfF/NS5uTBpUoe4TAee8NQgxhhTpxUVwZQpMHgwHHJITE+dm+tmdd2/vxOTJvk/aZ+1MIwxxk8+pgKZNQv27wcQ9u8Hv/MPWsAwxhg/TZwIrVvD+eGmB4rO9kAGPhGlfn3X2vCTBQxjjPFLMBXIlVfGPBVIfj5MmuSeAbzxxnW+d0eBjWEYY4x/pk71LRXI00+7nq4ZM2DPnu/o2bNzzD+jImthGGOMXyZOhJ//HE47Laan3bLFBYzLLov5qcOygGGMMX5Yvx7mzvUlFcijj8K+ffDIIzE9bbUsYBhjjB8mTXLvV18d09OuXw8vvQTXX+/yGMaTBQxjjIm1YCqQ3r1jngrkgQcgPd29x5sFDGOMibXFi+Grr2I+2L1ihYtDt9wC7dvH9NSeWMAwxphYC6YC+dWvYnrae++F5s3h7rtjelrPLGAYY0wsBVOBXHhhTFOBzJ/vHukYMQJatozZaSNiAcMYY2Lpgw9g69aYdkepulZFZiYMGxaz00bMHtwzxphYmjgRWrWKaSqQ9993k/W98AI0aRKz00bMWhjGGBMru3bB22+7VCD168fklKWlMHIkdO4MN90Uk1NGzbeAISLjRWSLiKyoYvtxIpIrIoUiMrzCtttFZKWIrBCRySLS0K9yGmNMzPiQCuS112DZMnj44ZjFoKj52cKYAIRrk+0AbgX+FLpSRI4IrO+uqicC6cCVPpXRGGNiZ+JEOOYY6NEjJqfbvx/uuw+yslyjJdF8CxiqOhcXFKravkVVFwFFlWyuBzQSkXpAY2CTP6U0xpgY2bABPv44pqlA/v53WLsWHn8c0pJgACHpBr1V9b8i8ifgO2AvMFNVZ1a1v4gMBYYCZGZmkhPlDCIFBQVRH5uKrL4iY/UVmdpYXx1eeYXOwIKjj2ZfDMq+d28a9913OllZe2nUaGnYyZHiVl+q6tsL6AisqGafUcDwkOVDgY+ANkAG8BZwtZfP69atm0Zrzpw5UR+biqy+ImP1FZlaV1+lparHHafau3fMTvnYY6qgOm9e9fvWpL6Axerxmu6pkSPO1SJyf2C5g4jEppPuYOcC61R1q6oWAW8AZ/r0WcYYU3NLlsCXX8ZssHvHDnjySffs31lnxeSUMeG1V+xFoCcwJLC8G3jBlxK5rqgzRKSxiAjwC2C1T59ljDE1F+NUIE8+6WbUe/TRmJwuZryOYZyuqqeKyOcAqvqjiIS9wUtEJgPZQGsRyQMewHUxoapjReRwYDHQHCgVkduALqq6UESmAp8BxcDnwLjIfzVjjImDoiKYPDlmqUD++18YM8ZlRT/ppBiUL4a8BowiEUkHFEBE2gCl4Q5Q1SHVbP8eqDTfoqo+gAswxhiT3P78Z5cKJEZT3z30EJSUwIMPxuR0MeW1S2oM8CZwmIg8CswDHvOtVMYYUxvk5paljh01yi3XwJo17lbam2+GTp1qXrxY89TCUNVJIrIEN54gwMWqauMKxpjU9q9/ueYAuKfscnKgZ8+oT/fHP0LDhi6NeTLyepfUz3B3Lr0ArAD6iUjs8vYaY0xt9O237j093eXtyM6O+lRLlsDrr8Mdd7istMnIa5fUNKBERI4G/gZ0Al71rVTGGJPsNm92aWQvvtglepo9u0ati3vucUlu77wzhmWMMa+D3qWqWiwivwT+rKp/Cd4xZYwxKWnMGCguhtGj4eija3Sqjz6CmTPh6aehRYsYlc8HXlsYRSIyBLgWmB5Yl+FPkYwxJsnl58NLL8Gll9Y4WMyfD7/+NbRpA7/7XYzK5xOvAeN63IN7j6rqOhHpBLziX7GMMSaJjRvn5r4YMaJGp8nNhb59IS8Pdu6Ez5O838ZTwFDVVap6q6pODiyvU9Un/C2aMcYkof374dln4ZxzoHv3Gp3qww/d6cBNlJTs+Ra93iU1SEQ+F5EdIpIvIrtFJN/vwhljTNKZNAk2bapx6wJg/Xr3npZW45us4sLroPdzwC+B5YHshsYYk3pKS90gd9eu0L9/jU71zTfw6qvQr5/rlsrOrtFNVnHhNWBsxKUpt2BhjEld06fD6tWulVGDSZJU4dZbXatiwgRo1y52RfST14AxApghIh8DhcGVqvqML6Uyxphk9NRTcNRRcPnlNTrNW2/Be+/BM8/UnmAB3gPGo0AB0BBI8DTkxhiTAP/5j3uNGQP1op+sdM8eGDbMZaK95ZYYli8OvP7WLVW1Zh12xhhTmz31lHsU+4YbanSaRx6BjRvd+EUN4k5CeH0OY5aIWMAwxqSmVavgnXfg97+HJk2iPs2XX7qnuX/9a+jVK4bli5NqA0Zg1rsRwPsistduqzXGpJw//QkaNXIBI0qq8H//5+LNU0/FsGxxVG2DSFVVRJaq6qnxKJAxxiSVvDx45RX47W+hdeuoT/Paay5n1AsvwGGHxbB8ceS1SypXRGIznZQxxtQmf/6ze/7ijjuiPkV+vju8WzcXd2orr0MufYGbRWQ9sAc3iZKqapZfBTPGmITbuRP++ld3G20NpsAbNQq+/97dTpueHrvixZvXgDHA11IYY0wyGjsWdu+Gu+6K+hTLl7s7cW+6CXr0iGHZEsDrFK0b/C6IMcYklX37XHdU//5wyilRnSI40H3IIfD44zEuXwJ4HcOImIiMF5EtIrKiiu3HiUiuiBSKyPAK2w4Rkaki8qWIrBaRJM+wYoypcyZOdP1INUgyOHEifPIJPPGEe4SjtvMtYAATgPPDbN8B3Ar8qZJtfwbeV9XjgK7A6piXzhhjqlJS4m6l7dbNpTGPws6drifrjDNq/Kxf0vDtOUNVnSsiHcNs3wJsEZGBoetFpDnQB7gusN9+YL9f5TTGmIO8/TasWePuhY0yyeB998G2bW7a7zQ/v5rHUTI+mN4Z2Ar8PxHpCiwBhqnqnsp2FpGhwFCAzMxMcqKcgaSgoCDqY1OR1VdkrL4ik9D6UuXUe+8lo107Pm3VCo2iHGvWNOWll7px0UX/Zdeub3yfGClu9aWqvr2Ajri06OH2GQUMD1nuDhQDpweW/ww87OXzunXrptGaM2dO1MemIquvyFh9RSah9ZWTowqqL74Y1eElJaqnn66aman6448xLlsValJfwGL1eE1PxoZSHpCnqgsDy1MBe8rcGBMfTz0FbdrAdddFdfjf/w4LF7p5lg45JLZFS7SkCxiq+j2wUUSODaz6BbAqgUUyxqSK5cthxgw3u1GjRhEfvn073H039O4NV1/tQ/kSzLcxDBGZDGQDrUUkD3gAyABQ1bEicjiwGGgOlIrIbUAXVc0HbgEmiUh9YC1wvV/lNMaYA0aPdtkBf/e7qA4fORJ27XL5omowIV/S8vMuqSHVbP8eaF/FtqW4sQxjjImP776DyZNdRtqWLSM+fOFC+Nvf4Pbb3eRIdVHSdUkZY0xCPPuse7/99ogPLSlxjZK2bV3eqLoqGW+rNcaY+NqxA15+GYYMgQ4dIj587Fj47DOYMgWaNfOhfEnCWhjGGPPii26y7SiSDL77Ltx5J3Tv7pLa1mUWMIwxqS0nxyV7OuOMiAcf5s+Hiy6CwkJYsQIWLPCniMnCAoYxJnXl5rpstHv2wOefu+UIjB7txi8Aiorw/YnuRLOAYYxJXdOnuys9QHFxRFf8L75wj2ykpblJkerXh+xsX0qZNGzQ2xiTuoJ9SBFe8QsK3HhFq1ZurPyLL9yhPev4RAwWMIwxqemNN+Cjj2DoUOjY0fMVX9XdQvv11zB7tjts4MBqD6sTLGAYY1LPjh3uqn/yyfD885CR4fnQCRPcxEgPPlj3u6AqsoBhjEk9t9/uEj+9/35EwWLVKjfl6jnnwL33+li+JGWD3saY1DJjBvzzny7x08knez7sp5/cuEWzZvDKK27YI9VYC8MYkzp27YLf/hZOOCHiJsKtt7oWxgcfuBQgqcgChjEmdYwYAZs2wbRp0KCB58MmTXLzXNzseuM9AAAX3ElEQVR7L/Tr52P5kpx1SRljUsPs2TBunMvj0aOH58PWrHGNkt6963ZiQS8sYBhj6r6CArjpJjjmGHd7k0f79rlxi4YN4dVXoV6K98mk+K9vjEkJ99wDGzbA3LkRzaR3xx2wbJlLMNi+0tl7Uou1MIwxddsnn8Bf/uImRurVy/Nhr78OL73kEthecIGP5atFLGAYY+quvXvhxhuhUyd4/HHPh337revBOuMMePRRH8tXy1iXlDGm7nrgAZfDY9YsN1e3B4WFcMUVbrxiypSInuur8yxgGGPqpk8/haefdrmifvELz4eNGAFLlsBbb8FRR/lYvlrIuqSMMXVPYSHccAO0awdPPeX5sLfegjFj4Lbb3MRIpjzfAoaIjBeRLSKyoortx4lIrogUisjwSrani8jnIjLdrzIaY+qoRx6BlSvhr3+FFi08HbJ+PVx/vZtq9ckn/S1ebeVnC2MCcH6Y7TuAW4E/VbF9GLA6xmUyxtR1n3/uBrivvdbz7U1FRXDllVBaCq+95qbGMAfzLWCo6lxcUKhq+xZVXQQUVdwmIu2BgcDf/CqfMaYOKipyXVFt2sCzz3o6JDfXDXEsXOjSf3Tu7HMZa7FkHfR+DhgBNKtuRxEZCgwFyMzMJCfKSXULCgqiPjYVWX1FxuorMtHWV4dXXqHz0qWsePhhtn3xRbX7r1zZnNtvP5miojTS0kr54Yel5OTkR1HixIrX31fSBQwRGQRsUdUlIpJd3f6qOg4YB9C9e3fNjnJGk5ycHKI9NhVZfUXG6isyUdXXypVuZqMrruDE++7zdMikSWVTeoukkZ9/aq2cFClef1/JeJfUWcBgEVkPTAHOEZFXElskY0xSKylxXVHNm7unuj34+GMXX0QintI7ZSVdC0NVRwIjAQItjOGqenVCC2WMSW7PPeeeu5g82Y1fVGPePDcPd+fOMHo0fPGF5ym9U5pvAUNEJgPZQGsRyQMeADIAVHWsiBwOLAaaA6UichvQRVVrXweiMSZxXn8d7r7b5R+/4opqd58/HwYMcMkEP/oIDj/cBQ9TPd8ChqoOqWb790DY/I+qmgPkxK5Uxpg65d133f2wqrBoESxYELaZsHAhnH++mzEvGCyMd8k4hmGMMdXbvNk9aafqlouKIMydQosWQf/+cNhhMGeOewjcRMYChjGm9snLg7PPdhMjNWhQ7aj1Z5+5YNGqlQsWRxwR3+LWFUk36G2MMWFt2ADnnAPbtrkstCKuZVHFqPXSpXDuuXDIIS5YHHlk3EtcZ1jAMMbUHmvXQt++kJ8PH35YNjd3FeMWX3zhgkWzZi5YWPbZmrGAYYypHb7+2gWLvXth9mw49dSwu69Y4VJ+NGrkgkXHjvEpZl1mAcMYk/xWr3ZX/+Jid/XPygq7+6pVrteqfn23u+WHig0LGMaY5BZsKgTHKrp0Cbv7l1+6YFGvngsWRx8dn2KmArtLyhiTvJYudYPZ9eq5XB7VBIs1a1ywAPecxc9/7n8RU4kFDGNMclq82F39Gzd2weLYY8Pu/s03boijpMQFi+OOi1M5U4gFDGNM0mm+apXrhmrRwgWLMP1KubkwfDiceSbs3+/Gw6tpiJgo2RiGMSa5zJtH1l13leXv6NChyl1zc10jZN8+t/zPf8KJJ8apnCnIWhjGmOSRkwPnn8/+Vq1cyyJMsAB44YWyYJGe7h4AN/6xgGGMSQ6zZrk5uI86iqXPPRc2f0dBAfzmN24CJJvPIn6sS8oYk1i5ufDXv8Krr8Lxx8OsWexfubLK3RctgquucoPcI0e67LP/+Y/NZxEPFjCMMYkzf767tWn/ftdUePLJKidAKilxmx94wA1vzJnj8g8C9OkTxzKnMOuSMsYkxpIlrqmwf79bTkuDzz+vdNcNG1xcufdeuPRSWLasLFiY+LGAYYyJr02b4Lrr4LTTYOdOyMgIOwgxZQp07eqe4fvnP90srIceGvdSGyxgGGPi5aef4KGH4Jhj3FX/rrtg/Xp3N9TDD7sHKEIGIfLz4dprYcgQ91zF0qVwzTWu58okho1hGGP8VVrqAsTdd7v7Xi+7zA1GBDMC9ux50Gj18uXNuf562LgRRo1yXVH17GqVcPZPYIzxz/z5cPvt8Omn0K2buxOqd+8qdy8udo2NRx45haOOgk8+sTufkolvXVIiMl5EtojIiiq2HyciuSJSKCLDQ9YfKSJzRGS1iKwUkWF+ldEY45MNG+DKK+Gss1yrYsIEFzTCBItvv3WbH3oI+vX7gaVLLVgkGz9bGBOA54F/VrF9B3ArcHGF9cXAnar6mYg0A5aIyIequsq3khpjYmP3bnj8cXjmGXfX0/33w4gR0KRJlYfMnw/PPgvvvuum554yBTIzv6R588PjWHDjhW8BQ1XnikjHMNu3AFtEZGCF9ZuBzYGfd4vIauAIwAKGMckoN9flfCoogP/3/+CHH+Dqq+Gxx8JOoK0KL70Et9zihjnS0tyT25dc4jKEmOST1GMYgYBzCrAwsSUxxlRq1iwYOLDsWYqTToJ33imba7sSpaXw73/DE0/AggVl60Xc5EcmeSVtwBCRpsA04DZVzQ+z31BgKEBmZiY5UX41KSgoiPrYVGT1FZm6VF+NNm6kVW4urRYsoMWyZaSVlgKgIqw7/XS+++mnSpsIRUXC7NmZTJlyJBs2NOHww/fyq19t5Z13jqCoSKhXT2nefBk5Ofl1qr7iIW71paq+vYCOwIpq9hkFDK+wLgP4ALgjks/r1q2bRmvOnDlRH5uKrL4iU6vra98+1ZkzVYcNUz36aFXXm6R6/PGq//M/qg0aqKanqzZqpDp//kGH5+erPv20avv27rCsLNVXX1UtKnLb589Xfeyx8ofW6vpKgJrUF7BYPV5jk66FISIC/B1YrarPxOVDc3PpMGmSG3GL5raM3Fz3jcqyn5m6YtMmmDHDjUR/+CHs2eP+f/TtC8OGuW6oTp3cvr//faV//1u2wF/+As8/7x7ozs6Gl1+G884r//BdJY9hmCTlW8AQkclANtBaRPKAB3AtB1R1rIgcDiwGmgOlInIb0AXIAq4BlovI0sDp7lHVGb4UNDcXevemU0mJG7A791yX2axePfdKTy/7ubLlvDz3v6C42C0PH+4eS83IcKkOKr5Xtm7ZMpeCs29flyAnPT3y38EClolWcND6sMPgu+9ckAjmdGrf3g1gDxzoZiqq7G6nClf8devgT3+C8eOhsBAuvhj+8Ac4/fQ4/T7GN37eJTWkmu3fA+0r2TQPiN/D/zk5UFLiPrCkxN0r3qyZCwDBV0nJwcuVKSpytxRG65FH3Hu9etCwoftG17Bh+VfFdbt3u//spaUu0Fx7rZv7uHHjsleTJuWXK76WLHHpGSzg1H2FhS4dx9q17vXJJ/Cvf7m/H3Bf/c88093hNHCgG8SuJhdH8PvKEUfA++/D66+7O56uvdZl/6hmKm5TiyRdl1TcZWdDo0aUFhaS1qCB+3ZV3UVT1f0HKy52ifgHDXJ3iWRkuPsCs7LcclGRew/9ueL7W2/BG2+4c4q4eYx79HDTiO3b5/6DB38OXc7Pd++bN5cFsOJi97WuJtq3d+mlmzYt/2rSpNzy4Xl57vbJpk3dBWjVKujVyz15FdzPay4HayHVTGj9nXEG7NjhnoJbu7bsPfhzXp77WwuqV68sWKSlwT33uEetPZozx81HEbxJqlEj92D3bbeFnf/I1FIWMHr2hNmzWT9+PJ1vuMHbBSs4xVd6umumz54d/QXvZz9zfcX797vuqYceiuwcubkuyASP//BDOOUUl+jtp59c33Pw59BXcP1777ljggGrZUv3P72gALZtc8GgoKDsVVQEwHGVleXFF8svN2hwcOCp+MrPd99wS0rcxeuuu1zAbdQofKuoQYOyb741DTjJenxRkauf4GvXrvLL+fkuUE+a5L4siLh6++mn8uc//HCXtyk727137uz+7jp3dv1H555b9vdzwQXVFnfDBve9avp096dTXOzWi7ge2YceirwKTO1gAQOgZ0++Kyykc7TfbmsyahcIWFFfcKo6vnFjb8effrrrlgheMMaODV+G/fthzx5yP/yQnied5EY0x44te/LqoovcbDahQabia9u2sp937Ci74hQVua4QL4IXx4wMd+EMBrxjjnG5r4PjRRVfoWNJ9evD1q0uMV5JifsCcP31cNRR7lxeXhs2uEAZHMO68UY3BlZc7H6foiIoLuaY9evd5wSWD2zbssW1UktL3fnatSsLFMHJqqurh2CLQdUF28svLwsInTqFfcqatm2r/fsrLnYxLRgkgpPhHX00/PKX8Pbbbp/69WHAAG//fKZ2soCRDGp6m0g8A1bgQlt42GFuOs2rr3Y3CwQDzl13Rd9Cysgom6azslbR3r0Hr/vkE1gYeK5T1QWtQw5x5wt23VXsHgx97d1bvkvv5ZejqUWnqMgFT3AX8owMF0QyMmgDZQEusI6MDNi+vaxLSBVatXJjCM2be3stW1a+hfDMM9F96ahwzPbtbjxi+nT44AP48UdX7D594IYbXC/sz3/u9rUexdRhAcMkZwvJq4pdcuPH16xL74MPXKur7GmD8K+FC91tQEVF7vj33nNjORXudJufk0N2JZMDHfT51bXwKjrzzJrVX6AIc+a4LB4bN7qWxIIFLo4ddphrNA4aBP36uRhVkd0WmzosYJiaq00tpFgff9557i61RH1+8BwRHvfjj65x8uab8MIL5W/869YN7rvPBYlu3VyjzRiwgGHqgkQGrGQ4PgxV12pYutQ9WrF0qXutX3/wvmlp7nkJr8NIJvVYwDCmlguOIfTq5cb7QwPD0qXuvgIouyfg9NPht791N9MVFbkx8mCP2IUXJvRXMUnOAoYxtUhpKXz/fdmjFXPnurmJKj5L2rChe+bu0kvh5JNdcDjpJHcnc0U17REzqcMChjEJVvEuo/x893jE2rVl78Gf161zz25WRsRNl/3AA+7paq/PTdqgtfHKAoYxcVRQ4B7O37TJvefmlj3GIeKy0uRXSObfvLl7pKJLFzcQ3alT2SMWmze7Z+2CXUq33w4nnJCY383UfRYwjKmh+fPd3bjHHw+ZmeUDQvDntWt78OOPLmBURRWOO851I4UGhUMPrTqd07HHWpeSiR8LGCblVUzF9NNP7mH0bdvcA2wVfw5dt2mTe1i8Mo0auQep27WDzp0L6Nq1Me3ala1r29aldrrkkrIWwnPPxeS5O2N8YQHDJFwsUzF16xY+9VLFdevWuRZCMDNHvXoH0mUdRMR922/d2j2Q3aGDez5v27ayh8yvvx7uvNMFhObNy1oGOTmryM4+7KBznnCCtRBM7WEBA3fBmTSpQ8LmT6pLx59+etWJeUPfgz8vW+YS1gVTMd15p/vmvXdvWSaQ0PeKPwfzI4YmYK1OvXruYt6ihTtHMDMHuBbGoEFlQSH0/dBDD56qpOKD2jfe6LqmImEtBFNbpHzACMyfRElJpwPzJ7VpU36fcBejrVvdN8Rg7rpzznEXl3DHhy5v23ZgSg7S0938SS1blu0Xmleusvft28u+IaeluQt2ixZlx5aWVv5zcDk/3yWTC37DPuYYd0tmSUnZq7S0/HJJCRQWnklamrtjZ88ez9Ud1v79B08n0qCB69oJJq8N/blVKxc8gnUh4v79LrywLNVSixYH/9ywYflEt6EX/CefjOziHYsHtY2pLVI+YAQv1iAH5k869NCD96tq0PHHH8vugS8pcXMRtWx58P5VLW/fXv745ctdwAluDyZFDT0m9P2HH8q+IZeWugnT2rVz29LSyidWDV1OT3fvP/1UPvddWpobaA1mbw++0tLKL3///VaOPPIIPvvM5R0KJos95xx3AQ6dVLCqyQczMuCrr+COO1wLI5h7sHdvFxQaNqx+8sGKF/wHH4z/Bd9aCCZleJ38uza8unXrFvEE6PPnu7nr09JKqprD3tPx6emaUscHJ52v6ecHz/HYY9EdG4vj4yFYX8Ybq6/I1KS+gMXq8Rqb8i2M4DfM8ePXc8MNneOeOy7Vjw+eI0lTMRljQqR8wAB3sSks/I6ePTtHfXxtzl2X6OONMbWDJS42xhjjiW8BQ0TGi8gWEVlRxfbjRCRXRApFZHiFbeeLyFci8o2I3O1XGY0xxnjnZwtjAnB+mO07gFuBP4WuFJF04AVgANAFGCIiXXwqozHGGI98CxiqOhcXFKravkVVFwEVn6vtAXyjqmtVdT8wBbjIr3IaY4zxJhnHMI4ANoYs5wXWGWOMSaBkvEuqskfkqnzWWkSGAkMBMjMzycnJiepDCwoKoj42FVl9RcbqKzJWX5GJV30lY8DIA44MWW4PbKpqZ1UdB4wDEJGtffv23RDY1ALYFbJrdcutgW3RFzusip8Vy2PC7VfVtsrWV7fO6iuydVZfka8LXbb6il99HeV5T69P+EXzAjoCK6rZZxQwPGS5HrAW6ATUB5YBJ0Tx2eMiXPb8tGNNyxLLY8LtV9W2ytZXt87qy+rLz/qqpP6svpKkvkJfvrUwRGQykA20FpE84AEgA0BVx4rI4cBioDlQKiK3AV1UNV9Efg98AKQD41V1ZRRF+HeEy36K5rO8HhNuv6q2Vba+unVWX5Gts/qKfF286szqK0oSiE4pT0QWq2r3RJejtrD6iozVV2SsviITr/pKxrukEmVcogtQy1h9RcbqKzJWX5GJS31ZC8MYY4wn1sIwxhjjiQUMY4wxnljAMMYY44kFDA9EpImILBGRQYkuS20gIseLyFgRmSoi/5vo8iQ7EblYRF4WkbdFpH+iy5PsRKSziPxdRKYmuizJKnDN+kfg7+qqWJ23TgeMqlKsR5E+/Q/A6/6UMrnEos5UdbWq3gxcDtTpWyNjVF9vqepvgOuAK3wsbsLFqL7WquqN/pY0+URYd78Epgb+rgbHqgx1OmBQSYr1qtKni8hJIjK9wuswETkXWAX8EO/CJ8gEalhngWMGA/OA2fEtftxNIAb1FXBf4Li6bAKxq69UMwGPdYdLqRRM4loSqwIkYy6pmFHVuSLSscLqA+nTAURkCnCRqj4OHNTlJCJ9gSa4f4y9IjJDVUt9LXgCxaLOAud5B3hHRN4FXvWvxIkVo78xAZ4A3lPVz/wtcWLF6u8rFUVSd7icfO2BpcSwYVCnA0YVKkuffnpVO6vqvQAich2wrS4HizAiqjMRycY1iRsAM3wtWXKKqL6AW4BzgRYicrSqjvWzcEko0r+vVsCjwCkiMjIQWFJVVXU3BnheRAYSwxQiqRgwIkqffmAH1QmxL0qtEVGdqWoOkONXYWqBSOtrDO4/eKqKtL62Azf7V5xapdK6U9U9wPWx/rC6PoZRmYjSpxvA6ixSVl+RsfqKXlzrLhUDxiLgGBHpJCL1gSuBdxJcpmRndRYZq6/IWH1FL651V6cDhrgU67nAsSKSJyI3qmoxEEyfvhp4Pcr06XWS1VlkrL4iY/UVvWSoO0s+aIwxxpM63cIwxhgTOxYwjDHGeGIBwxhjjCcWMIwxxnhiAcMYY4wnFjCMMcZ4YgHDmDBEpCBG5xklIsM97DdBRC6LxWcaE2sWMIwxxnhiAcMYD0SkqYjMFpHPRGS5iFwUWN9RRL4Ukb+JyAoRmSQi54rIf0TkaxHpEXKariLyUWD9bwLHi4g8LyKrAqngDwv5zPtFZFHgvOMCadCNSRgLGMZ4sw+4RFVPBfoCT4dcwI8G/gxkAccB/wP0AoYD94ScIwsYCPQE7heRdsAlwLHAScBvgDND9n9eVU9T1ROBRtjcECbBUjG9uTHREOAxEekDlOLmIcgMbFunqssBRGQlMFtVVUSWAx1DzvG2qu7FTcQ1Bzf5TR9gsqqWAJtE5KOQ/fuKyAigMdASWEkM5zYwJlIWMIzx5iqgDdBNVYtEZD3QMLCtMGS/0pDlUsr/H6uYuE2rWI+INAReBLqr6kYRGRXyecYkhHVJGeNNC2BLIFj0BY6K4hwXiUjDwIxx2bjU1HOBK0UkXUTa4rq7oCw4bBORpoDdOWUSzloYxngzCfi3iCzGzZP8ZRTn+BR4F+gAPKyqm0TkTeAcYDmwBvgYQFV3isjLgfXrccHFmISy9ObGGGM8sS4pY4wxnljAMMYY44kFDGOMMZ5YwDDGGOOJBQxjjDGeWMAwxhjjiQUMY4wxnljAMMYY48n/B3bkDeKQ3R4QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1, m_X, s = standardize(x_clean.copy())\n",
    "x1 = x1[0:68110]\n",
    "y1 = y_clean[0:68110].copy()\n",
    "print(\"Least-Square-GD\")\n",
    "#cross_validation_LS_GD_demo(x1,y1,5)\n",
    "print(\"Least-Square-SGD\")\n",
    "#cross_validation_LS_SGD_demo(x1,y1,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x1,y1)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x1,y1,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= x_nan.copy()\n",
    "y2= y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "#cross_validation_LS_GD_demo(x2,y2,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "#cross_validation_LS_SGD_demo(x2,y2,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x2,y2)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x2,y2,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x2,y2,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x2,y2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n",
      "   gamma=0.234, mse=0.694\n",
      "   accuracy=0.740\n",
      "Least-Square-SDG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2bb4ae2222c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcross_validation_LS_GD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-SDG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcross_validation_LS_SGD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcompute_least_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-c07b188cc0bc>\u001b[0m in \u001b[0;36mcross_validation_LS_SGD_demo\u001b[1;34m(x_LS, y_LS, K)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0msgd_mse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mmse_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mleast_squares_SGD\u001b[1;34m(y, tx, initial_w, batch_size, max_iters, gamma)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x3 = x_subgroups\n",
    "y3 = y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x3,y3,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x3,y3,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x3,y3)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x3,y3,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,2)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x3,y3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 =x_s.copy()\n",
    "y4 =y_s.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x4,y4,4)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x4,y4,4)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x4,y4)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x4,y4,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x4,y4,4)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x4,y4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and Save output in CSV format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
