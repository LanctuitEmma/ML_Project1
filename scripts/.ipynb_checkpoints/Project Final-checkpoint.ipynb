{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project on Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "- We only use imported data without any modification\n",
    "- We then cleaned it by removing all samples containing at least one Nan value\n",
    "- We then partioned our dataset into 4 subgroups, depending on the value of feature PRI_jet_num. We then standardized each subgroup individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*\n",
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nan values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the Higgs dataset, we saw that there were a lot of missing values, corresponding to the -999.0 value. We thus decided to remove the samples that had at least one -999.0 outlier value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 72.7544 % of our training dataset.\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "print(\"We removed\", (1-x_clean.shape[0]/x.shape[0])*100, \"% of our training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that too many data samples are lost in x_clean. So it is not a good idea to remove these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values in dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we chose an alternative way to deal with the Nan values. We decided to replace each NaN value by the mean of the feature it is in. The mean was computed without taking into account the Nan values in the feature. This is a standardization concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_NAN(x):\n",
    "    x_nan = x.copy()\n",
    "    x_nan[x_nan==-999.0] = np.nan\n",
    "    return (standardize(x_nan))\n",
    "\n",
    "\n",
    "# All the Nan (corresponding to unknown values) were replaced by the mean value of the feature it is in.\n",
    "def replace_mean(x_nan):\n",
    "    means_cols = np.nanmean(x_nan, axis=1)\n",
    "    for row in range(x_nan.shape[0]):\n",
    "        for col in range(x_nan.shape[1]):\n",
    "            if np.isnan(x_nan[row,col]):\n",
    "                x_nan[row,col]=means_cols[col]\n",
    "    return (x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the outliers of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assumed that the dataset can have some ouliers. So to deal with them we implemented some methods that can remove the datasamples where some percentile is trespassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionning of dataset, based on PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing some data analysis, we saw that a specific column was only composed of 4 discrete values. This column was the \"PRI_jet_num\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names and their respective indices\n",
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus partitionned our dataset in the 4 different subgroups, corresponding to the value of the PRI_jet_num. This way, when doing the standardization on the dataset, we didn't bias our samples: indeed, the 4 groups have different kind of means and standard deviations values, which we used when standardizing each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subgrouping\n",
    "def subgrouping(x,ids,dict):\n",
    "    x_0=x[x[:,dict['PRI_jet_num']]==0]\n",
    "    x_1=x[x[:,dict['PRI_jet_num']]==1]\n",
    "    x_2=x[x[:,dict['PRI_jet_num']]==2]\n",
    "    x_3=x[x[:,dict['PRI_jet_num']]==3]\n",
    "    x_list = [x_0]\n",
    "    x_list.append(x_1)\n",
    "    x_list.append(x_2)\n",
    "    x_list.append(x_3)\n",
    "\n",
    "    ids_0=ids[x[:,dict['PRI_jet_num']]==0]\n",
    "    ids_1=ids[x[:,dict['PRI_jet_num']]==1]\n",
    "    ids_2=ids[x[:,dict['PRI_jet_num']]==2]\n",
    "    ids_3=ids[x[:,dict['PRI_jet_num']]==3]\n",
    "    ids_list = [ids_0]\n",
    "    ids_list.append(ids_1)\n",
    "    ids_list.append(ids_2)\n",
    "    ids_list.append(ids_3)\n",
    "\n",
    "    #Standardization of subgroups\n",
    "    mean = []\n",
    "    std = []\n",
    "    x_nan_replaced = []\n",
    "    for i in range(4):\n",
    "        x_arr,m,s = standardize_NAN(x_list[i])\n",
    "        x_nan_replaced.append(replace_mean(x_arr))\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return x_nan_replaced, ids_list\n",
    "    \n",
    "#Grouping them back again\n",
    "def group(l,ids):\n",
    "    ls = l.copy()\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    return data_ord[data_ord[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subgroups_list, ids_list = subgrouping(x,ids,dict)\n",
    "x_subgroups = group(x_subgroups_list, ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying all features that had Nan Values, we saw that many sample of our dataset had to replace their Nan value ba the mean value of the feature, as explained above in the standardization part. The problem is that having a mean value for 75% of our datasample is bad for some features. That's why we decided to remove some of them, and play only with the features that are essential to our model.\n",
    "\n",
    "Indeed, the columns that were removed were all features that derived from some primitive ones. And as most of them had many Nan values, we assumed it was a good idea to remove them and see how our model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    x_selected = np.zeros((len(x), len(selected_columns0)))\n",
    "    for i in range(len(x)):\n",
    "        s = np.take(x[i], indices=selected_columns0, axis=0)\n",
    "        x_selected[i] = s\n",
    "    return x_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = selected_non_nan_columns(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build polynomial features, meaning we expanded the number of features we had by adding features with an incremeneted degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    nb_features = x.shape[1]\n",
    "    nb_samples = x.shape[0]\n",
    "    x_poly = np.ones((nb_samples, 1))\n",
    "    for d in range(1, degree+1):\n",
    "        x_d = x**d\n",
    "        x_poly = np.hstack((x_poly, x_d))\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_s, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of the different ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least Squares Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\"   gamma={l:.3f}, mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least Squares Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 50\n",
    "    max_batch_size = 32\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "        result_mse_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LS[ind]\n",
    "                tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "                mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_mse_gamma.append(np.mean(mse_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_mse.append(np.min(result_mse_gamma))\n",
    "        result_gamma.append(gammas[np.argmin(result_mse_gamma)])\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_mse_gamma)])\n",
    "\n",
    "    print(\"   gamma={l:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = np.nanmin(result_mse), l =result_gamma[np.nanargmin(result_mse)], b=np.nanargmin(result_mse)))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "    \n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_squares(x, y):\n",
    "    x_LS= x.copy()\n",
    "    y_LS= y.copy()\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "    opt_w = []\n",
    "    K_values = return_factors(len(x_LS))\n",
    "    #K-fold crossvalidation\n",
    "    for K in K_values:\n",
    "        #Initialization\n",
    "        list_x_LS = np.split(x_LS,K)\n",
    "        list_y_LS = np.split(y_LS,K)\n",
    "        for ind, x_bloc in enumerate(list_x_LS):\n",
    "            x_test = x_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            x_train = np.concatenate(list_x_LS[:ind] + list_x_LS[ind+1:])\n",
    "            y_train = np.concatenate(list_y_LS[:ind] + list_y_LS[ind+1:])\n",
    "            mse_LS, optimal_weights_LS = least_squares(y_train,x_train)\n",
    "            mse_errors.append(compute_mse(y_test, x_test, optimal_weights_LS))\n",
    "            weights.append(optimal_weights_LS)\n",
    "\n",
    "    opt_w = weights[np.argmin(mse_errors)]\n",
    "    y_model = predict_labels(opt_w, x_LS)\n",
    "\n",
    "    #Computing accuracy\n",
    "    print(\"   mse={mse}\".format(mse = mse_LS))\n",
    "    accuracy = (list(y_model == y_LS).count(True))/len(y_model)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo_RR(x,y,K=4):\n",
    "    seed = 1\n",
    "    degree = 4\n",
    "    k_fold = K\n",
    "    lambdas = np.logspace(-4, 0, 20)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    x_test = x[k_indices[0]]\n",
    "    x_train = np.delete(x, [k_indices[0]], axis=0)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for i in range(len(lambdas)):\n",
    "        l = lambdas[i]\n",
    "        avg_err_tr = 0\n",
    "        avg_err_te = 0\n",
    "        for k in range(k_fold):\n",
    "            err = cross_validation_rr(y, x, k_indices, k, l, degree)\n",
    "            avg_err_tr += err[0]\n",
    "            avg_err_te += err[1]\n",
    "        rmse_tr.append(np.sqrt(2 * avg_err_tr / k_fold))\n",
    "        rmse_te.append(np.sqrt(2 * avg_err_te / k_fold))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    min_err_index = 0\n",
    "    for i in range(1, len(rmse_te)):\n",
    "        if rmse_te[i] < rmse_te[min_err_index]:\n",
    "            min_err_index = i\n",
    "            \n",
    "    print('Best lambda is: {0}'.format(lambdas[min_err_index]))       \n",
    "    return lambdas[min_err_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x_s,x,y,degree_opt=4):\n",
    "    x_poly = build_poly(x_s, degree_opt)\n",
    "    lambda_opt = cross_validation_demo_RR(x,y,K)\n",
    "    w_rr_opt, loss_tr = ridge_regression_s(y_s, x_poly, lambda_opt)\n",
    "    print(\"Training set mse: {}\".format(loss_tr))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_clean == y_predicted).count(True))/len(y_clean))\n",
    "    print(\"accuracy = {val}\".format(val=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "\n",
    "    list_tX_LR = np.split(tX_LR,K)\n",
    "    list_y_LR = np.split(y_LR,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LR):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LR[ind]\n",
    "            tX_train= list_tX_LR[:ind] + list_tX_LR[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LR[:ind] + list_y_LR[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "            loss, opt_w = logistic_regression(y_train,tX_train,w_initial, max_iters, gamma)\n",
    "            loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\"   gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR, tX_LR)\n",
    "    accuracy = (list(y_predicted == y_LR).count(True))/len(y_LR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "\n",
    "    max_iters = 50\n",
    "    lambdas = np.logspace(-4,0,10)\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    list_tX_LRR = np.split(tX_LRR,K)\n",
    "    list_y_LRR = np.split(y_LRR,K)\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind,lambda_ in enumerate(lambdas):  \n",
    "        result_loss_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LRR):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LRR[ind]\n",
    "                tX_train= list_tX_LRR[:ind] + list_tX_LRR[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LRR[:ind] + list_y_LRR[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                loss, opt_w = reg_logistic_regression(y_train,tX_train,lambda_,w_initial,max_iters,gamma)\n",
    "                loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_loss_gamma.append(np.mean(loss_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_loss.append(np.min(result_loss_gamma))\n",
    "        result_gamma.append(np.argmin(result_loss_gamma))\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_loss_gamma)])\n",
    "\n",
    "    del result_loss_gamma\n",
    "    del result_opt_w_gamma\n",
    "    del loss_errors\n",
    "    del weights\n",
    "    print(np.min(result_loss))\n",
    "    print(result_gamma[np.argmin(result_loss)])\n",
    "    print(np.argmin(result_loss))\n",
    "    print(\"   gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_loss), l =result_gamma[np.argmin(result_loss)], b=np.argmin(result_loss)))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "    print(optimal_weights_LRR)\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR, tX_LRR)\n",
    "    accuracy = (list(y_predicted == y_LRR).count(True))/len(y_LRR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for cleaned dataset (with removed samples containing Nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, m_X, s = standardize(x_clean.copy())\n",
    "x1 = x1[0:68110]\n",
    "y1 = y_clean[0:68110].copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x1,y1,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x1,y1,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x1,y1)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x1,y1,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= x_nan.copy()\n",
    "y2= y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x2,y2,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x2,y2,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x2,y2)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x2,y2,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x2,y2,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x2,y2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonxh\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py:118: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - gamma*grad\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All-NaN slice encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fc63098556ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-GD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcross_validation_LS_GD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-SDG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcross_validation_LS_SGD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-1330bde4b8df>\u001b[0m in \u001b[0;36mcross_validation_LS_GD_demo\u001b[1;34m(x_LS, y_LS, K)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mgen_mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mgen_opt_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0moptimal_gamma_LS_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanargmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0moptimal_weights_LS_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_opt_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanargmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmse_LS_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py\u001b[0m in \u001b[0;36mnanargmin\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All-NaN slice encountered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All-NaN slice encountered"
     ]
    }
   ],
   "source": [
    "x3 = x_subgroups\n",
    "y3 = y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x3,y3,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x3,y3,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x3,y3)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x3,y3,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x3,y3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 =x_s.copy()\n",
    "y4 =y_s.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x4,y4,4)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x4,y4,4)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x4,y4)\n",
    "#print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x4,y4,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x4,y4,4)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x4,y4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and Save output in CSV format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
