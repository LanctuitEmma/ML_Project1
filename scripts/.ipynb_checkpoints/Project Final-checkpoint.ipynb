{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project on Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "- We only use imported data without any modification\n",
    "- We then cleaned it by removing all samples containing at least one Nan value\n",
    "- We then partioned our dataset into 4 subgroups, depending on the value of feature PRI_jet_num. We then standardized each subgroup individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nan values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the Higgs dataset, we saw that there were a lot of missing values, corresponding to the -999.0 value. We thus decided to remove the samples that had at least one -999.0 outlier value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 72.7544 % of our training dataset.\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "x_clean,_,_ = standardize(x_clean)\n",
    "print(\"We removed\", (1-x_clean.shape[0]/x.shape[0])*100, \"% of our training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that too many data samples are lost in x_clean. So it is not a good idea to remove these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values in dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we chose an alternative way to deal with the Nan values. We decided to replace each NaN value by the mean of the feature it is in. The mean was computed without taking into account the Nan values in the feature. This is a standardization concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_NAN(x):\n",
    "    x_nan = x.copy()\n",
    "    x_nan[x_nan==-999.0] = np.nan\n",
    "    return (standardize(x_nan))\n",
    "\n",
    "# All the Nan (corresponding to unknown values) were replaced by the mean value of the feature it is in.\n",
    "def replace_mean(x_nan):\n",
    "    means_cols = np.nanmean(x_nan, axis=1)\n",
    "    is_nan = np.isnan(x_nan)\n",
    "    for col in range(x_nan.shape[1]):\n",
    "        x_nan[is_nan[:, col], col] = means_cols[col]\n",
    "    return (x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the outliers of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assumed that the dataset can have some ouliers. So to deal with them we implemented some methods that can remove the datasamples where some percentile is trespassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionning of dataset, based on PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing some data analysis, we saw that a specific column was only composed of 4 discrete values. This column was the \"PRI_jet_num\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names and their respective indices\n",
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus partitionned our dataset in the 4 different subgroups, corresponding to the value of the PRI_jet_num. This way, when doing the standardization on the dataset, we didn't bias our samples: indeed, the 4 groups have different kind of means and standard deviations values, which we used when standardizing each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subgrouping\n",
    "def subgrouping(x,ids,dict):\n",
    "    x_0=x[x[:,dict['PRI_jet_num']]==0]\n",
    "    x_1=x[x[:,dict['PRI_jet_num']]==1]\n",
    "    x_2=x[x[:,dict['PRI_jet_num']]==2]\n",
    "    x_3=x[x[:,dict['PRI_jet_num']]==3]\n",
    "    x_0 = np.delete(x_0,dict['PRI_jet_num'],1)\n",
    "    x_1 = np.delete(x_1,dict['PRI_jet_num'],1)\n",
    "    x_2 = np.delete(x_2,dict['PRI_jet_num'],1)\n",
    "    x_3 = np.delete(x_3,dict['PRI_jet_num'],1)\n",
    "    x_list = [x_0, x_1, x_2, x_3]\n",
    "\n",
    "    ids_0=ids[x[:,dict['PRI_jet_num']]==0]\n",
    "    ids_1=ids[x[:,dict['PRI_jet_num']]==1]\n",
    "    ids_2=ids[x[:,dict['PRI_jet_num']]==2]\n",
    "    ids_3=ids[x[:,dict['PRI_jet_num']]==3]\n",
    "    ids_list = [ids_0]\n",
    "    ids_list.append(ids_1)\n",
    "    ids_list.append(ids_2)\n",
    "    ids_list.append(ids_3)\n",
    "    \n",
    "\n",
    "    #Standardization of subgroups\n",
    "    mean = []\n",
    "    std = []\n",
    "    x_nan_replaced = []\n",
    "    for i in range(4):\n",
    "        x_arr,m,s = standardize_NAN(x_list[i])\n",
    "        x_nan_replaced.append(replace_mean(x_arr))\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return x_nan_replaced, ids_list\n",
    "    \n",
    "#Grouping them back again\n",
    "def group(l,ids,dict):\n",
    "    ls = l.copy()\n",
    "    for i in range(4):\n",
    "        ls[i] = np.insert(ls[i],dict['PRI_jet_num'],np.ones((len(ids),1))*i,axis=1)\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    x_new = data_ord[data_ord[:,0].argsort()]\n",
    "    x_new = x_new[:,1:]\n",
    "        \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subgroups_list, ids_list = subgrouping(x,ids,dict)\n",
    "x_subgroups = group(x_subgroups_list, ids_list,dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying all features that had Nan Values, we saw that many sample of our dataset had to replace their Nan value ba the mean value of the feature, as explained above in the standardization part. The problem is that having a mean value for 75% of our datasample is bad for some features. That's why we decided to remove some of them, and play only with the features that are essential to our model.\n",
    "\n",
    "Indeed, the columns that were removed were all features that derived from some primitive ones. And as most of them had many Nan values, we assumed it was a good idea to remove them and see how our model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    return x[:,selected_columns0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = selected_non_nan_columns(x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build polynomial features, meaning we expanded the number of features we had by adding features with an incremeneted degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    nb_features = x.shape[1]\n",
    "    nb_samples = x.shape[0]\n",
    "    x_poly = np.ones((nb_samples, 1))\n",
    "    for d in range(1, degree+1):\n",
    "        x_d = x**d\n",
    "        x_poly = np.hstack((x_poly, x_d))\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_s, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of the different ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least Squares Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "    seed=1\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for gamma in gammas:\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        for k in range(K):\n",
    "            mse_te, opt_w = cross_validation_ls_GD(y_LS, tX_LS,k_indices,k, gamma,max_iters,w_initial)\n",
    "            mse_errors.append(mse_te)\n",
    "            weights.append([opt_w])\n",
    "        \n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "        \n",
    "    del weights\n",
    "    del mse_errors\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\"   gamma={l:.3f}, mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD.T, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted.flatten()).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n",
    "    \n",
    "    del gen_opt_w\n",
    "    del gen_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least Squares Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    seed=1\n",
    "    max_iters = 50\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "    \n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    \n",
    "    hyperparams = [(batch_size,gamma) for batch_size in batch_sizes for gamma in gammas ]\n",
    "    \n",
    "    \n",
    "    for batch_size,gamma in hyperparams:  \n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            \n",
    "            for k in range(K):\n",
    "                mse_te, opt_w = cross_validation_ls_SGD(y_LS, tX_LS, k_indices, k, gamma, max_iters, w_initial,batch_size)\n",
    "                mse_errors.append(mse_te)\n",
    "                weights.append([opt_w])\n",
    "    \n",
    "            result_mse.append(np.mean(mse_errors))\n",
    "            result_opt_w.append(np.mean(weights,axis=0))\n",
    "            \n",
    "    del mse_errors\n",
    "    del weights\n",
    "    \n",
    "    mse = np.min(result_mse)\n",
    "    hyper_opt= hyperparams[np.argmin(result_mse)]\n",
    "    print(\"   gamma={g:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = mse, g=hyper_opt[1], b=hyper_opt[0]))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD.T, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted.flatten()).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del result_mse\n",
    "    del result_opt_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_squares(x_LS, y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "    \n",
    "    seed = 1\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "    \n",
    "    for k in range(K):\n",
    "            mse_te, opt_w = cross_validation_ls(y_LS, tX_LS,k_indices,k)\n",
    "            mse_errors.append(mse_te)\n",
    "            weights.append([opt_w])\n",
    "    \n",
    "    mse_LS = np.min(mse_errors)\n",
    "    opt_w = weights[np.argmin(mse_errors)]\n",
    "    y_model = predict_labels(np.array(opt_w).T, tX_LS)\n",
    "\n",
    "    #Computing accuracy\n",
    "    print(\"   mse={mse}\".format(mse = mse_LS))\n",
    "    accuracy = (list(y_model.flatten() == y_LS).count(True))/len(y_model)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del mse_errors\n",
    "    del weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo_RR(x,y,K=4):\n",
    "    # Investigate the rmse loss with ridge regression for a given degree and different lambdas\n",
    "    # Print the best lambda, according to the rmse loss of the testing set (using cross-validation)\n",
    "    degree_opt = 4\n",
    "    lambda_opt= ridge_regression_invest(degree_opt, y, x)\n",
    "    \n",
    "    x_poly = build_poly(x, degree_opt)\n",
    "    w_rr_opt, loss_tr = ridge_regression(y, x_poly, lambda_opt)\n",
    "    print(\"Training set mse: {}\".format(loss_tr))\n",
    "    \n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y == y_predicted).count(True))/len(y))\n",
    "    print(\"accuracy = {val}\".format(val=accuracy))\n",
    "    \n",
    "    del w_rr_opt\n",
    "    del x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-2,-1,5)\n",
    "    seed=1\n",
    "    \n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LR, K, seed)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for gamma in gammas:\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        \n",
    "        for k in range(K):\n",
    "            loss_te, opt_w = cross_validation_lr(y_LR, tX_LR, k_indices, k, gamma, max_iters, w_initial)\n",
    "            loss_errors.append(loss_te)\n",
    "            weights.append([opt_w])\n",
    "    \n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights,axis=0))\n",
    "    \n",
    "    del weights\n",
    "    del loss_errors\n",
    "        \n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\"   gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR.T, tX_LR)\n",
    "    accuracy = (list(y_predicted.flatten() == y_LR).count(True))/len(y_LR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del gen_opt_w\n",
    "    del gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = build_poly(x_LRR, 4)\n",
    "    #tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "    seed = 1\n",
    "    max_iters = 50\n",
    "    lambdas = np.logspace(-1,0,10)\n",
    "    gammas = np.logspace(-1,0,20)\n",
    "    hyperparams = [(gamma,lambda_) for gamma in gammas for lambda_ in lambdas]\n",
    "\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LRR, K, seed)\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    for gamma,lambda_ in hyperparams:  \n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            \n",
    "            for k in range(K):\n",
    "                loss_te, opt_w = cross_validation_lrr(y_LRR, tX_LRR, k_indices, k, lambda_, gamma, max_iters, w_initial)\n",
    "                loss_errors.append(loss_te)\n",
    "                weights.append([opt_w])\n",
    "    \n",
    "            result_loss.append(np.mean(loss_errors))\n",
    "            result_opt_w.append(np.mean(weights,axis=0))\n",
    "\n",
    "    \n",
    "    del loss_errors\n",
    "    del weights\n",
    "    mse = np.min(result_loss)\n",
    "    hyper_opt= hyperparams[np.argmin(result_loss)]\n",
    "    print(\"   gamma={g:.3f}, mse={mse:.3f} lambda{l:.3f}\".format(mse = mse, g=hyper_opt[0], l=hyper_opt[1]))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "   \n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR.T, tX_LRR)\n",
    "    accuracy = (list(y_predicted.flatten() == y_LRR).count(True))/len(y_LRR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del result_loss\n",
    "    del result_opt_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for cleaned dataset (with removed samples containing Nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n",
      "Least-Square-SGD\n",
      "Least-Square\n",
      "Ridge Regression\n",
      "Logistic Regression\n",
      "Regularized Logistic Regression\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ab33aa421d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#cross_validation_LR_demo(x1,y1,5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularized Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcross_validation_LRR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-4826efd1dc87>\u001b[0m in \u001b[0;36mcross_validation_LRR_demo\u001b[1;34m(x_LRR, y_LRR, K)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_lrr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_LRR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_LRR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mloss_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Etudes\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_lrr\u001b[1;34m(y, x, k_indices, k, lambda_, gamma, max_iters, w_initial)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Etudes\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Etudes\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_loss_logistic_reg\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#we added 1e-5 to our sigmoid so we don't get a log(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x1, m_X, s = standardize(x_clean.copy())\n",
    "x1 = x1[0:68110]\n",
    "y1 = y_clean[0:68110].copy()\n",
    "print(\"Least-Square-GD\")\n",
    "#cross_validation_LS_GD_demo(x1,y1,5)\n",
    "print(\"Least-Square-SGD\")\n",
    "#cross_validation_LS_SGD_demo(x1,y1,5)\n",
    "print(\"Least-Square\")\n",
    "#compute_least_squares(x1,y1,5)\n",
    "print(\"Ridge Regression\")\n",
    "#cross_validation_demo_RR(x1,y1,K=5)\n",
    "print(\"Logistic Regression\")\n",
    "#cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "   gamma=0.234,loss=5875.093\n",
      "   accuracy=0.726\n",
      "Regularized Logistic Regression\n",
      "   gamma=0.234, mse=5937.926 lambda0.000\n",
      "   accuracy=0.725\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n",
      "   gamma=0.234, mse=0.681\n",
      "   accuracy=0.744\n",
      "Least-Square-SDG\n",
      "   gamma=0.234, batch=2.00, mse=0.684\n",
      "   accuracy=0.742\n",
      "Least-Square\n",
      "   mse=0.6772157957149298\n",
      "   accuracy=0.744\n",
      "Ridge Regression\n",
      "Best lambda is: 0.0001\n",
      "Training set mse: 0.6118240927582305\n",
      "accuracy = [0.7994098129606249]\n",
      "Logistic Regression\n",
      "   gamma=0.234,loss=12886.583\n",
      "   accuracy=0.744\n",
      "Regularized Logistic Regression\n",
      "   gamma=0.234, mse=13157.366 lambda0.000\n",
      "   accuracy=0.742\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2= x_nan.copy()\n",
    "y2= y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x2,y2,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x2,y2,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x2,y2,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x2,y2,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x2,y2,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x2,y2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n",
      "   gamma=0.234, mse=0.694\n",
      "   accuracy=0.740\n",
      "Least-Square-SDG\n",
      "   gamma=0.234, batch=2.00, mse=0.697\n",
      "   accuracy=0.737\n",
      "Least-Square\n",
      "   mse=0.6865312772396522\n",
      "   accuracy=0.743\n",
      "Ridge Regression\n",
      "Best lambda is: 0.002395026619987486\n",
      "Training set mse: 0.6125056307858856\n",
      "accuracy = [0.7989253310626303]\n",
      "Logistic Regression\n",
      "   gamma=0.234,loss=33621.107\n",
      "   accuracy=0.740\n",
      "Regularized Logistic Regression\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8c30c6d0a24a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcross_validation_LR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularized Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcross_validation_LRR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-5658ac9101de>\u001b[0m in \u001b[0;36mcross_validation_LRR_demo\u001b[1;34m(x_LRR, y_LRR, K)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_lrr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_LRR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_LRR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mloss_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_lrr\u001b[1;34m(y, x, k_indices, k, lambda_, gamma, max_iters, w_initial)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_loss_logistic_reg\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_loss_logistic_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x3 = x_subgroups\n",
    "y3 = y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x3,y3,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x3,y3,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x3,y3,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x3,y3,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,2)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x3,y3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "   gamma=0.234,loss=13449.286\n",
      "   accuracy=0.740\n",
      "Regularized Logistic Regression\n",
      "   gamma=0.234, mse=13752.378 lambda0.000\n",
      "   accuracy=0.737\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x3,y3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with removed nan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-fa7774ad4036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mx4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-GD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcross_validation_LS_GD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcompute_least_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-77e63d2fc81a>\u001b[0m in \u001b[0;36mcross_validation_LS_GD_demo\u001b[1;34m(x_LS, y_LS, K)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmse_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mmse_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_ls_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_LS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mmse_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_ls_GD\u001b[1;34m(y, x, k_indices, k, gamma, max_iters, w_initial)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m     \u001b[0mmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[0mmse_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmse_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\MA1\\ML\\ML_Project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;34m\"\"\"Compute the gradient.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# w is (n_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# e shape is n_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x4 =x_s.copy()\n",
    "y4 =y.copy()\n",
    "#Normalizing some columns with huge values, such that gradient doesn't explode\n",
    "for i in range(x4.shape[1]):\n",
    "    if max(x4[:,i]) > 200:\n",
    "        x4[:,i] = x4[:,i]/np.linalg.norm(x4[:,i])\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x4,y4,4)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x4,y4,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x4,y4,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x4,y4,4)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x4,y4,4)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x4,y4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and Save output in CSV format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
