{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(tX != -999.0, axis=1)\n",
    "tX_clean = tX[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(tX.shape)\n",
    "print(tX_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = list(y).count(1)\n",
    "count_2 = list(y).count(-1)\n",
    "print(\"Nombre de label = 1: \", count_1 , \"\\nNombre de label = -1: \" , count_2)\n",
    "print(\"Pourcentage de label 1: \" , count_1/len(y), \"\\nPourcentage de label -1: \", count_2/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the boxplots of each features, but taking care that all Nan values per previously removes (tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(num, tX_clean):\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    data = []\n",
    "    for i in range(0, num):\n",
    "        data.append(tX_clean[:, i])\n",
    "    ax1.boxplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below is computing the indices of our tX dataset, where for the x_j_th datapoint, we have values not in the bounds of the percentile. Thus we store these j_th indexes, such we can remove them. This method computes this fot the i_th feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX_perc[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below thus removes all rows where the row has a value not in the percentile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_by_percentiles(tX,tX_clean,y):\n",
    "    args = []\n",
    "    for i in range(30):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.7)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    tX_perc = tX.copy()\n",
    "    y_perc = y.copy()\n",
    "    tX_perc = np.delete(tX_perc, mylist, axis=0)\n",
    "    y_perc = np.delete(y_perc,mylist,axis=0)\n",
    "    return tX_perc,y_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are plotting the correlation between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_of_2_features(y, tX, feat1, feat2):\n",
    "    y_copy = y.copy()\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    colormap = np.array(['r', 'g', 'b'])\n",
    "    #We change values -1 of y to 0 \n",
    "    y_copy[y_copy<0] = 0\n",
    "    categories = np.array(y_copy.copy())\n",
    "    categories = categories.astype(int)\n",
    "    plt.scatter(tX[:,feat1], tX[:,feat2],  c=colormap[categories])\n",
    "    plt.title(features[feat1] + ' and ' + features[feat2])\n",
    "    \n",
    "plot_of_2_features(y,tX,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    #print(np.nanstd(tX_nan, axis=0))\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "tX_nan, mean_x_nan, std_x_nan = standardize_NAN(tX)\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplacÃ©s par la moyenne de la colonnes\n",
    "means_cols = np.nanmean(tX_nan,axis=1)\n",
    "for row in range(0,tX_nan.shape[0]):\n",
    "    for col in range(0,tX_nan.shape[1]):\n",
    "        if np.isnan(tX_nan[row,col]):\n",
    "            tX_nan[row,col]=means_cols[col]\n",
    "            \n",
    "tX_poly = tX_nan.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are adding new features by doing some polynomial computations with some features having the biggest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new features\n",
    "x1,_,_=standardize(tX_poly[:,9]+tX_poly[:,21]**2+tX_poly[:,29]**3)\n",
    "x2,_,_=standardize(tX_poly[:,5]+tX_poly[:,9]**2+tX_poly[:,29])\n",
    "x3,_,_=standardize(tX_poly[:,5]**3+tX_poly[:,9]**2+tX_poly[:,21])\n",
    "\n",
    "\n",
    "tX_poly= np.column_stack((tX_poly,x1))\n",
    "tX_poly= np.column_stack((tX_poly,x2))\n",
    "tX_poly= np.column_stack((tX_poly,x3))\n",
    "\n",
    "tX_poly = np.delete(tX_poly, [4,6,27,28], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can use a regression model to predict the missing value per feature, whithout taking into acount the outliers of a particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_par_feat(tX,y):\n",
    "    tX_model = tX.copy()\n",
    "    y_model = y.copy()\n",
    "    dic = {}\n",
    "    #finds all datapoints where at feature \"ind\" it has value -999.0 and stores it in a dictionnary\n",
    "    for ind, tX_ in enumerate(tX_model.T):\n",
    "        dic[ind] = list(np.where(tX_ == -999.0)[0])\n",
    "nan_par_feat(tX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_feature(tX,y):\n",
    "    tX_model = tX.copy()\n",
    "    y_model = y.copy()\n",
    "    dic = {}\n",
    "    #finds all datapoints where at feature \"ind\" it has value -999.0 and stores it in a dictionnary\n",
    "    for ind, tX_ in enumerate(tX_model.T):\n",
    "        dic[ind] = list(np.where(tX_ == -999.0)[0])\n",
    "    #We are gonna try to group features depending on the indices where they have -999.0\n",
    "    ls = {}\n",
    "    for ind in range(len(dic)):\n",
    "        for ind2 in range(ind+1,len(dic)):\n",
    "            sum_ = len(set(dic[ind]) & set(dic[ind2])) \n",
    "            if sum_ in ls:\n",
    "                if ind2 not in ls[sum_]:\n",
    "                    ls[sum_].append(ind2)\n",
    "            else:\n",
    "                ls[sum_] = [ind,ind2]\n",
    "    return ls\n",
    "dic = model_feature(tX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the k value of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_factors(x):\n",
    "    # This function takes a number and prints the factors\n",
    "    a = []\n",
    "    for i in range(2,10):\n",
    "        if x % i == 0:\n",
    "            a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_LS=tX_clean.copy()\n",
    "#y_LS= y_clean.copy()\n",
    "#accuracy=0.7239627683002026 mse=0.7401139564470155\n",
    "\n",
    "#tX_LS=tX_nan.copy()\n",
    "#y_LS= y.copy()\n",
    "# accuracy=0.745088 mse=0.6786542286532609\n",
    "\n",
    "def compute_least_squares(tX, y):\n",
    "    tX_LS=tX.copy()\n",
    "    y_LS= y.copy()\n",
    "\n",
    "    K_values = return_factors(len(tX_LS))\n",
    "    accuracy = []\n",
    "    #K-fold crossvalidation\n",
    "    for K in K_values:\n",
    "        #Initialization\n",
    "        list_tX_LS = np.split(tX_LS,K)\n",
    "        list_y_LS = np.split(y_LS,K)\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        opt_w = []\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train = np.concatenate(list_tX_LS[:ind] + list_tX_LS[ind+1:])\n",
    "            y_train = np.concatenate(list_y_LS[:ind] + list_y_LS[ind+1:])\n",
    "            mse_LS, optimal_weights_LS = least_squares(y_train,tX_train)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test, optimal_weights_LS))\n",
    "            weights.append(optimal_weights_LS)\n",
    "\n",
    "        opt_w = weights[np.argmin(mse_errors)]\n",
    "        y_model = predict_labels(opt_w, tX_LS)\n",
    "\n",
    "        #Computing accuracy\n",
    "        accuracy.append((list(y_model == y_LS).count(True))/len(y_model))\n",
    "        print(\"accuracy = {val} mse={mse}\".format(mse = mse_LS, val=accuracy[-1]))\n",
    "\n",
    "    #Plot of accuracies\n",
    "    print(\"\\nMaximum accuracy = {val}\".format(val=np.max(accuracy)))\n",
    "    #plt.plot(K_values, accuracy, '.-', markersize=15, label = \"Accuracy\");\n",
    "    #plt.xlabel(\"K value\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Accuracies for Least Squares\")\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows were deleted based on the percentile concept\n"
     ]
    }
   ],
   "source": [
    "tx_perc, y_perc = remove_rows_by_percentiles(tX_nan,tX_clean,y)\n",
    "print(tX.shape[0]-tx_perc.shape[0], \"rows were deleted based on the percentile concept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.745008 mse=0.6781761342388203\n",
      "accuracy = 0.745088 mse=0.6786542286532609\n",
      "accuracy = 0.744904 mse=0.6786166979058809\n",
      "accuracy = 0.745044 mse=0.6785307962457391\n",
      "\n",
      "Maximum accuracy = 0.745088\n"
     ]
    }
   ],
   "source": [
    "compute_least_squares(tX_nan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.745008 mse=0.6781761342388203\n",
      "accuracy = 0.745088 mse=0.6786542286532609\n",
      "accuracy = 0.744904 mse=0.6786166979058809\n",
      "accuracy = 0.745044 mse=0.6785307962457391\n",
      "\n",
      "Maximum accuracy = 0.745088\n"
     ]
    }
   ],
   "source": [
    "compute_least_squares(tx_perc,y_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './leastSquarePOLY' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(opt_w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
