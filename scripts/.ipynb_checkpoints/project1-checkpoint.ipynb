{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(x.shape)\n",
    "print(x_clean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplacÃ©s par la moyenne de la colonnes\n",
    "def replace_mean(tX_nan):\n",
    "    means_cols = np.nanmean(tX_nan,axis=1)\n",
    "    for row in range(0,tX_nan.shape[0]):\n",
    "        for col in range(0,tX_nan.shape[1]):\n",
    "            if np.isnan(tX_nan[row,col]):\n",
    "                tX_nan[row,col]=means_cols[col]\n",
    "    return (tX_nan)\n",
    "\n",
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)\n",
    "\n",
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)\n",
    "\n",
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    x_selected = np.zeros((len(x), len(selected_columns0)))\n",
    "    for i in range(len(x)):\n",
    "        s = np.take(x[i], indices=selected_columns0, axis=0)\n",
    "        x_selected[i] = s\n",
    "    return x_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234948, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sel = selected_non_nan_columns(x_perc)\n",
    "x_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = x_sel != -999\n",
    "v.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = x_sel\n",
    "y_s = y_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  6.60090000e+01,  1.49462000e+02,\n",
       "         1.36136000e+02,  2.80100000e+00,  3.46930000e+01,\n",
       "         2.72747000e+02,  3.24000000e-01,  4.86000000e-01,\n",
       "         8.10770000e+01, -1.30900000e+00,  1.24600000e+00,\n",
       "         2.63040000e+01,  1.03900000e+00, -2.82000000e-01,\n",
       "         5.96570000e+01,  1.68800000e+00,  3.42194000e+02,\n",
       "         1.00000000e+00,  1.65366000e+02,  4.35718808e+03,\n",
       "         2.23388894e+04,  1.85330105e+04,  7.84560100e+00,\n",
       "         1.20360425e+03,  7.43909260e+04,  1.04976000e-01,\n",
       "         2.36196000e-01,  6.57347993e+03,  1.71348100e+00,\n",
       "         1.55251600e+00,  6.91900416e+02,  1.07952100e+00,\n",
       "         7.95240000e-02,  3.55895765e+03,  2.84934400e+00,\n",
       "         1.17096734e+05,  1.00000000e+00,  2.73459140e+04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_poly = build_poly(x_s, 2)\n",
    "lambda_ = 0.1\n",
    "w, loss = ridge_regression_s(y_s, x_poly, lambda_)\n",
    "x_poly[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT RUN\n",
    "indices = build_k_indices(y_s, 4, seed=1)\n",
    "x_test = x_s[indices[0]]\n",
    "x_train = np.delete(x_s, [indices[0]], axis=0)\n",
    "x_train.shape\n",
    "loss_tr, loss_te = cross_validation_rr(y_s, x_s, k_indices=indices, k=1, lambda_=0.1, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_demo_RR(x,y,K):\n",
    "    seed = 1\n",
    "    degree = 4\n",
    "    k_fold = K\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    x_test = x[k_indices[0]]\n",
    "    x_train = np.delete(x, [indices[0]], axis=0)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for i in range(len(lambdas)):\n",
    "        l = lambdas[i]\n",
    "        avg_err_tr = 0\n",
    "        avg_err_te = 0\n",
    "        for k in range(k_fold):\n",
    "            err = cross_validation_rr(y, x, k_indices, k, l, degree)\n",
    "            avg_err_tr += err[0]\n",
    "            avg_err_te += err[1]\n",
    "        rmse_tr.append(np.sqrt(2 * avg_err_tr / k_fold))\n",
    "        rmse_te.append(np.sqrt(2 * avg_err_te / k_fold))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    min_err_index = 0\n",
    "    for i in range(1, len(rmse_te)):\n",
    "        if rmse_te[i] < rmse_te[min_err_index]:\n",
    "            min_err_index = i\n",
    "            \n",
    "    print('Best lambda is: {0}'.format(lambdas[min_err_index]))       \n",
    "    \n",
    "cross_validation_demo_RR(x_s,y_s,4)\n",
    "\n",
    "degree_opt = 4\n",
    "lambda_opt = 0.0006723357536499335\n",
    "x_poly = build_poly(x_s, degree_opt)\n",
    "w_rr_opt, loss_tr = ridge_regression_s(y_s, x_poly, lambda_opt)\n",
    "print(\"Training set mse: {}\".format(loss_tr))\n",
    "\n",
    "#Training Accuracy\n",
    "y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "accuracy = []\n",
    "accuracy.append((list(y_clean == y_predicted).count(True))/len(y_clean))\n",
    "print(\"accuracy = {val}\".format(val=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Least squares gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can alter on gamma + w_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\" gamma={l:.3f},mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LS == y_predicted).count(True))/len(y_LS))\n",
    "    print(accuracy)\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823, mse= 0.825, gamma = 0.0055\n",
    "#With tX_NAN : accuracy= 0.705, mse= 0.737, gamma = 0.089\n",
    "#With x_te_s: accuracy = 0.702, mse = 0.759, gamma = 0.144\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the gamma and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 50\n",
    "    max_batch_size = 32\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "        result_mse_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LS[ind]\n",
    "                tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "                mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_mse_gamma.append(np.mean(mse_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_mse.append(np.min(result_mse_gamma))\n",
    "        result_gamma.append(gammas[np.argmin(result_mse_gamma)])\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_mse_gamma)])\n",
    "\n",
    "    print(\" gamma={l:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = np.nanmin(result_mse), l =result_gamma[np.nanargmin(result_mse)], b=np.nanargmin(result_mse)))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "    \n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LS == y_predicted).count(True))/len(y_LS))\n",
    "    print(accuracy)\n",
    "\n",
    "#With tX_CLEAN : accuracy = , mse= , gamma = , batch=\n",
    "#With tX_NAN : accuracy= , mse= , gamma = ,batch=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (not regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "\n",
    "    list_tX_LR = np.split(tX_LR,K)\n",
    "    list_y_LR = np.split(y_LR,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LR):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LR[ind]\n",
    "            tX_train= list_tX_LR[:ind] + list_tX_LR[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LR[:ind] + list_y_LR[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "            loss, opt_w = logistic_regression(y_train,tX_train,w_initial, max_iters, gamma)\n",
    "            loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "\n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\" gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR, tX_LR)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LR == y_predicted).count(True))/len(y_LR))\n",
    "    print(accuracy)\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823, loss= , gamma = \n",
    "#With tX_NAN : accuracy= 0.34, loss= nan, gamma = 0.114\n",
    "#With x_te_s : accuracy = 0.7015, loss = 16775, gamma = 0.144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression regularized with lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "\n",
    "    max_iters = 50\n",
    "    lambdas = np.logspace(-4,0,10)\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    list_tX_LRR = np.split(tX_LRR,K)\n",
    "    list_y_LRR = np.split(y_LRR,K)\n",
    "\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind,lambda_ in enumerate(lambdas):  \n",
    "        result_loss_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LRR):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LRR[ind]\n",
    "                tX_train= list_tX_LRR[:ind] + list_tX_LRR[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LRR[:ind] + list_y_LRR[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                loss, opt_w = reg_logistic_regression(y_train,tX_train,lambda_,w_initial,max_iters,gamma)\n",
    "                loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_loss_gamma.append(np.mean(loss_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_loss.append(np.min(result_loss_gamma))\n",
    "        result_gamma.append(np.argmin(result_loss_gamma))\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_loss_gamma)])\n",
    "\n",
    "    print(np.min(result_loss))\n",
    "    print(result_gamma[np.argmin(result_loss)])\n",
    "    print(np.argmin(result_loss))\n",
    "    print(\" gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_loss), l =result_gamma[np.argmin(result_loss)], b=np.argmin(result_loss)))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "    print(optimal_weights_LRR)\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR, tX_LRR)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LRR == y_predicted).count(True))/len(y_LRR))\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "print(\"dataset1\")\n",
    "x, m_X,s = standardize(x_clean.copy())\n",
    "x = x_LS[0:68110]\n",
    "y = y_clean[0:68110].copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,5)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,5)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With nan value replaced\n",
    "print(\"dataset2\")\n",
    "x=x_nan.copy()\n",
    "y=y.copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,5)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,5)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With x_te\n",
    "print(\"dataset3\")\n",
    "x =x_s.copy()\n",
    "y =y_s.copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,4)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,4)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,4)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
