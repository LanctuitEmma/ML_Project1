{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(tX != -999.0, axis=1)\n",
    "tX_clean = tX[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(tX.shape)\n",
    "print(tX_clean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "\n",
    "tX_nan, mean_x_nan, std_x_nan = standardize_NAN(tX)\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplac√©s par la moyenne de la colonnes\n",
    "means_cols = np.nanmean(tX_nan,axis=1)\n",
    "for row in range(0,tX_nan.shape[0]):\n",
    "    for col in range(0,tX_nan.shape[1]):\n",
    "        if np.isnan(tX_nan[row,col]):\n",
    "            tX_nan[row,col]=means_cols[col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Least squares gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can alter on gamma + w_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.10663740969926\n",
      " gamma=0.089,mse=0.737\n",
      "[ 1.52629330e-01 -5.21742771e-01 -8.76666214e-02  2.07713589e-01\n",
      "  4.53561528e-02  6.52049190e-02  2.56568950e-04  7.61931716e-02\n",
      " -2.41081693e-02  1.21985027e-02  6.35830904e-02  9.00370519e-02\n",
      "  1.22605731e-02  2.79511696e-01  7.55293944e-02  7.49160617e-02\n",
      " -4.62982543e-02  7.56155755e-02  7.62740270e-02 -4.51089772e-02\n",
      "  7.62550201e-02 -3.86426824e-02  7.41566184e-02 -5.67454513e-02\n",
      " -1.75254759e-02  2.72325373e-02 -3.92475999e-02 -3.09312665e-02\n",
      "  2.56323668e-02 -6.94045947e-02]\n",
      "0.705004\n"
     ]
    }
   ],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "tX_LS, m_X,s = standardize(tX_clean)\n",
    "tX_LS = tX_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "\n",
    "#With nan value replaced\n",
    "tX_LS=tX_nan\n",
    "y_LS= y\n",
    "\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-4,0,20)\n",
    "K=5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "gen_opt_w=[]\n",
    "gen_mse =[]\n",
    "\n",
    "#gamma selection\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "    #K-fold crossvalidation\n",
    "    for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "        tX_test = tX_bloc\n",
    "        y_test = list_y_LS[ind]\n",
    "        tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "        tX_train= np.concatenate(tX_train)\n",
    "        y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "        y_train=np.concatenate(y_train)\n",
    "        \n",
    "        mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "        mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "        weights.append(opt_w)\n",
    "    gen_mse.append(np.mean(mse_errors))\n",
    "    gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "optimal_gamma_LS_GD = gammas[np.argmin(gen_mse)]\n",
    "optimal_weights_LS_GD = gen_opt_w[np.argmin(gen_mse)]\n",
    "print(\" gamma={l:.3f},mse={mse:.3f}\".format(mse = np.min(gen_mse), l = optimal_gamma_LS_GD))\n",
    "\n",
    "#Training Accuracy\n",
    "y_model = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "sum_ = 0\n",
    "for i,v in enumerate(y_model):\n",
    "    if(v == y_LS[i]):\n",
    "        sum_ = sum_+1\n",
    "print(sum_/len(y_model))\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823\n",
    "#With tX_NAN : accuracy= 0.705\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the gamma and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-9fc4cd2246b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mmse_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mresult_mse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind_gamma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mresult_opt_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind_gamma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "\n",
    "#With nan value replaced\n",
    "tX_LS=tX_nan\n",
    "y_LS= y\n",
    "\n",
    "max_iters = 50\n",
    "max_batch_size = 10\n",
    "gammas = [0.001,0.01]\n",
    "batch_sizes = np.arange(max_batch_size)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "result_mse = np.array((batch_sizes.shape[0], len(gammas)))\n",
    "result_opt_w =np.array((batch_sizes.shape[0], len(gammas)))\n",
    "\n",
    "print(result_mse.shape)\n",
    "print(result_opt_w)\n",
    "\n",
    "for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "    for ind_gamma,gamma in enumerate(gammas):\n",
    "        mse_errors=[]\n",
    "        weights=[]\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        result_mse[ind_batch,ind_gamma]= np.mean(mse_errors)\n",
    "        result_opt_w[ind_batch,ind_gamma]= np.mean(weights,axis=0)\n",
    "\n",
    "print(result_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './leastSquareGD' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(optimal_weights_LS_GD, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
