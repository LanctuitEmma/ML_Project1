{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "x_clean = tX[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(tX.shape)\n",
    "print(tX_clean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.01244882e+00,  7.45942342e-02,  5.73386172e-01,\n",
       "        -1.81164648e-01, -4.73599409e-01,  8.63811591e-01,\n",
       "        -4.54629500e-01, -4.50329942e-01, -3.04856665e-02,\n",
       "         1.65295333e+00, -4.66339853e-01, -4.68349195e-01,\n",
       "        -4.81269474e-01, -1.30844696e-01, -4.72443497e-01,\n",
       "        -4.79314148e-01,  7.42809499e-02, -4.58875043e-01,\n",
       "        -5.09508280e-01, -3.01681907e-01, -4.86422462e-01,\n",
       "         2.31163913e+00, -4.61824237e-01,  2.45064146e-01,\n",
       "        -4.60203801e-01, -4.78633564e-01,  1.41735614e-02,\n",
       "        -4.70034449e-01, -5.10167257e-01,  7.42667764e-01],\n",
       "       [ 1.25515779e+00,  2.59464425e-01,  6.31808307e-01,\n",
       "         3.66868248e-02, -1.95616781e-01,  1.16748267e-01,\n",
       "         1.24970362e-01, -4.45911552e-01, -4.60981610e-01,\n",
       "         8.68629689e-01, -4.73934299e-01, -4.68154742e-01,\n",
       "         8.04047905e-02, -2.95566163e-02, -4.61402924e-01,\n",
       "        -5.15957617e-01, -8.46082432e-02, -4.78017799e-01,\n",
       "        -4.82317357e-01, -4.96789750e-04, -5.04128431e-01,\n",
       "         1.29414549e+00, -4.72627147e-01,  1.59452385e-02,\n",
       "        -4.75597947e-01, -4.70920287e-01, -1.50381523e-01,\n",
       "         4.43622343e-01, -4.99933726e-02,  1.59452385e-02],\n",
       "       [-2.20367110e-02,  1.26849938e+00,  8.77228805e-01,\n",
       "        -9.84683761e-02, -1.95616781e-01,  1.16748267e-01,\n",
       "         1.24970362e-01, -4.49422497e-01, -3.82574093e-01,\n",
       "         1.65353668e+00, -4.42638270e-01, -4.68154742e-01,\n",
       "         8.04047905e-02, -1.36073304e-01, -4.91046108e-01,\n",
       "        -5.06040546e-01,  8.28140384e-01, -4.93725229e-01,\n",
       "        -4.72065396e-01,  1.02984280e-01, -5.07045216e-01,\n",
       "         2.32979882e+00, -4.72627147e-01, -5.39050776e-03,\n",
       "        -4.61251683e-01, -5.05338357e-01, -1.50381523e-01,\n",
       "         4.43622343e-01, -4.99933726e-02, -5.39050776e-03],\n",
       "       [ 1.07116264e+00,  3.96110427e-01,  3.90989848e-01,\n",
       "        -4.78957652e-01, -1.95616781e-01,  1.16748267e-01,\n",
       "         1.24970362e-01, -4.47672426e-01, -4.78957652e-01,\n",
       "         3.37245373e-01, -4.58000007e-01, -4.97311795e-01,\n",
       "         8.04047905e-02, -2.38776565e-01, -5.01308872e-01,\n",
       "        -4.83322027e-01,  9.25918815e-02, -4.89069175e-01,\n",
       "        -5.16919076e-01, -1.47654023e-01, -4.82781882e-01,\n",
       "         4.46289942e-01, -4.83430056e-01,  3.93324717e-01,\n",
       "        -6.66554280e-02, -2.66164101e-01, -1.50381523e-01,\n",
       "         4.43622343e-01, -4.99933726e-02, -4.83430056e-01]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "\n",
    "tX_nan, mean_x_nan, std_x_nan = standardize_NAN(tX)\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplac√©s par la moyenne de la colonnes\n",
    "means_cols = np.nanmean(tX_nan,axis=1)\n",
    "for row in range(0,tX_nan.shape[0]):\n",
    "    for col in range(0,tX_nan.shape[1]):\n",
    "        if np.isnan(tX_nan[row,col]):\n",
    "            tX_nan[row,col]=means_cols[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01244882,  0.07459423,  0.57338617, ..., -0.47003445,\n",
       "        -0.51016726,  0.74266776],\n",
       "       [ 1.25515779,  0.25946442,  0.63180831, ...,  0.44362234,\n",
       "        -0.04999337,  0.01594524],\n",
       "       [-0.02203671,  1.26849938,  0.8772288 , ...,  0.44362234,\n",
       "        -0.04999337, -0.00539051],\n",
       "       ...,\n",
       "       [ 0.65581237,  0.17042684,  0.3358518 , ...,  0.44362234,\n",
       "        -0.04999337, -0.02979428],\n",
       "       [ 0.542317  , -0.27426412,  0.25993975, ...,  0.44362234,\n",
       "        -0.04999337, -0.48343006],\n",
       "       [-0.02203671,  0.30254643,  0.28175083, ...,  0.44362234,\n",
       "        -0.04999337, -0.48343006]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Least squares gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can alter on gamma + w_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "tX_LS, m_X,s = standardize(tX_clean)\n",
    "tX_LS = tX_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "\n",
    "#With nan value replaced\n",
    "#tX_LS=tX_nan\n",
    "#y_LS= y\n",
    "\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-4,0,20)\n",
    "K=5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "gen_opt_w=[]\n",
    "gen_mse =[]\n",
    "\n",
    "#gamma selection\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "    #K-fold crossvalidation\n",
    "    for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "        tX_test = tX_bloc\n",
    "        y_test = list_y_LS[ind]\n",
    "        tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "        tX_train= np.concatenate(tX_train)\n",
    "        y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "        y_train=np.concatenate(y_train)\n",
    "        \n",
    "        mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "        mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "        weights.append(opt_w)\n",
    "    gen_mse.append(np.mean(mse_errors))\n",
    "    gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "optimal_gamma_LS_GD = gammas[np.argmin(gen_mse)]\n",
    "optimal_weights_LS_GD = gen_opt_w[np.argmin(gen_mse)]\n",
    "print(\" gamma={l:.3f},mse={mse:.3f}\".format(mse = np.min(gen_mse), l = optimal_gamma_LS_GD))\n",
    "\n",
    "#Training Accuracy\n",
    "y_model = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "sum_ = 0\n",
    "for i,v in enumerate(y_model):\n",
    "    if(v == y_LS[i]):\n",
    "        sum_ = sum_+1\n",
    "print(sum_/len(y_model))\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823, mse=\n",
    "#With tX_NAN : accuracy= 0.705, mse=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the gamma and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "\n",
    "#With nan value replaced\n",
    "tX_LS=tX_nan\n",
    "y_LS= y\n",
    "\n",
    "max_iters = 50\n",
    "max_batch_size = 5\n",
    "gammas = [0.001,0.01]\n",
    "batch_sizes = np.arange(max_batch_size)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX_LS.shape[1])\n",
    "list_tX_LS = np.split(tX_LS,K)\n",
    "list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "result_mse =[]\n",
    "result_opt_w=[]\n",
    "result_gamma=[]\n",
    "for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "    result_mse_gamma = []\n",
    "    result_opt_w_gamma = []\n",
    "    for ind_gamma,gamma in enumerate(gammas):\n",
    "        mse_errors=[]\n",
    "        weights=[]\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "    \n",
    "        result_mse_gamma.append(np.mean(mse_errors))\n",
    "        result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "    result_mse.append(np.min(result_mse_gamma))\n",
    "    result_gamma = np.argmin(result_mse_gamma)\n",
    "    result_opt_batch_w = weights[np.argmin(result_mse_gamma)]\n",
    "\n",
    "\n",
    "print(\" gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_mse), l =result_gamma[np.argmin(result_mse)], b=np.argmin(result_mse))\n",
    "\n",
    "optimal_weights_LS_SGD = result_opt_w[np.argmin(result_mse)]\n",
    "print(optimal_weights_LS_SDG)\n",
    "\n",
    "#Training Accuracy\n",
    "y_model = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "sum_ = 0\n",
    "for i,v in enumerate(y_model):\n",
    "    if(v == y_LS[i]):\n",
    "        sum_ = sum_+1\n",
    "print(sum_/len(y_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.38470e+02,  5.16550e+01,  9.78270e+01,  2.79800e+01,\n",
       "         9.10000e-01,  1.24711e+02,  2.66600e+00,  3.06400e+00,\n",
       "         4.19280e+01,  1.97760e+02,  1.58200e+00,  1.39600e+00,\n",
       "         2.00000e-01,  3.26380e+01,  1.01700e+00,  3.81000e-01,\n",
       "         5.16260e+01,  2.27300e+00, -2.41400e+00,  1.68240e+01,\n",
       "        -2.77000e-01,  2.58733e+02,  2.00000e+00,  6.74350e+01,\n",
       "         2.15000e+00,  4.44000e-01,  4.60620e+01,  1.24000e+00,\n",
       "        -2.47500e+00,  1.13497e+02]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from implementations import *\n",
    "\n",
    "x_poly = build_poly(tX_clean, 2)\n",
    "lambda_ = 0.1\n",
    "w, loss = ridge_regression_s(y_clean, x_poly, lambda_)\n",
    "loss\n",
    "x[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51086, 30)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = build_k_indices(y_clean, 4, seed=1)\n",
    "x_test = x_clean[indices[0]]\n",
    "x_train = np.delete(x_clean, [indices[0]], axis=0)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6620184379434025"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_tr, loss_te = cross_validation_rr(y_clean, x_clean, k_indices=indices, k=1, lambda_=0.1, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-134-67686820e24c>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-134-67686820e24c>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    err = cross_validation_rr(y_clean, x_clean¬£, k_indices, k, l, degree)\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from implementations import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 4\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_clean, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for i in range(len(lambdas)):\n",
    "        l = lambdas[i]\n",
    "        avg_err_tr = 0\n",
    "        avg_err_te = 0\n",
    "        for k in range(k_fold):\n",
    "            err = cross_validation_rr(y_clean, x_clean, k_indices, k, l, degree)\n",
    "            avg_err_tr += err[0]\n",
    "            avg_err_te += err[1]\n",
    "        rmse_tr.append(np.sqrt(2 * avg_err_tr / k_fold))\n",
    "        rmse_te.append(np.sqrt(2 * avg_err_te / k_fold))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    min_err_index = 0\n",
    "    for i in range(1, len(rmse_te)):\n",
    "        if rmse_te[i] < rmse_te[min_err_index]:\n",
    "            min_err_index = i\n",
    "            \n",
    "    print('Best lambda is: {0}'.format(lambdas[min_err_index]))       \n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './leastSquareGD' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(optimal_weights_LS_GD, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
