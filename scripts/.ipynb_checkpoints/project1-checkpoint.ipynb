{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y[y<0] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleanning dataset by deleting index where there is outsider (values -999.0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "\n",
    "print(x.shape)\n",
    "print(x_clean.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Too many data lost in tX_clean, maybe we can make calculation without taking into account the -999.0 in the average\n",
    "We can replace this value by NaN wich will be not taking into account during the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.56765498468643\n"
     ]
    }
   ],
   "source": [
    "def standardize_NAN(tX):\n",
    "    tX_nan = tX.copy()\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if (tX_nan[i,j] == -999.0):\n",
    "                tX_nan[i,j] = np.nan\n",
    "    return (standardize(tX_nan))\n",
    "\n",
    "\n",
    "# Tout les nans (correspondant a des valeurs non connues) sont remplacÃ©s par la moyenne de la colonnes\n",
    "def replace_mean(tX_nan):\n",
    "    means_cols = np.nanmean(tX_nan,axis=1)\n",
    "    for row in range(0,tX_nan.shape[0]):\n",
    "        for col in range(0,tX_nan.shape[1]):\n",
    "            if np.isnan(tX_nan[row,col]):\n",
    "                tX_nan[row,col]=means_cols[col]\n",
    "    return (tX_nan)\n",
    "\n",
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)\n",
    "\n",
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgroup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.83469273209879\n",
      "63.38124719688545\n",
      "129.81575662840595\n",
      "139.61904053684216\n"
     ]
    }
   ],
   "source": [
    "#Feature names\n",
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind\n",
    "    \n",
    "#Subgrouping\n",
    "x_0=x[x[:,dict['PRI_jet_num']]==0]\n",
    "x_1=x[x[:,dict['PRI_jet_num']]==1]\n",
    "x_2=x[x[:,dict['PRI_jet_num']]==2]\n",
    "x_3=x[x[:,dict['PRI_jet_num']]==3]\n",
    "y_0=y[x[:,dict['PRI_jet_num']]==0]\n",
    "y_1=y[x[:,dict['PRI_jet_num']]==1]\n",
    "y_2=y[x[:,dict['PRI_jet_num']]==2]\n",
    "y_3=y[x[:,dict['PRI_jet_num']]==3]\n",
    "ids_0=ids[x[:,dict['PRI_jet_num']]==0]\n",
    "ids_1=ids[x[:,dict['PRI_jet_num']]==1]\n",
    "ids_2=ids[x[:,dict['PRI_jet_num']]==2]\n",
    "ids_3=ids[x[:,dict['PRI_jet_num']]==3]\n",
    "x_list = [x_0]\n",
    "x_list.append(x_1)\n",
    "x_list.append(x_2)\n",
    "x_list.append(x_3)\n",
    "ids_list = [ids_0]\n",
    "ids_list.append(ids_1)\n",
    "ids_list.append(ids_2)\n",
    "ids_list.append(ids_3)\n",
    "\n",
    "#Standardization of subgroups\n",
    "mean = []\n",
    "std = []\n",
    "x_nan_replaced = []\n",
    "for i in range(4):\n",
    "    x,m,s = standardize_NAN(x_list[i])\n",
    "    x_nan_replaced.append(replace_mean(x))\n",
    "    mean.append(m)\n",
    "    std.append(s)\n",
    "    \n",
    "#Grouping them back again\n",
    "def group(ls,ids):\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    x_inorder = data_ord[data_ord[:,0].argsort()]\n",
    "    return x_inorder[:,1:]\n",
    "x_inorder = group(x_nan_replaced, ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)\n",
    "\n",
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    x_selected = np.zeros((len(x), len(selected_columns0)))\n",
    "    for i in range(len(x)):\n",
    "        s = np.take(x[i], indices=selected_columns0, axis=0)\n",
    "        x_selected[i] = s\n",
    "    return x_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sel = selected_non_nan_columns(x_perc)\n",
    "x_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = x_sel != -999\n",
    "v.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = x_sel\n",
    "y_s = y_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_s, 2)\n",
    "lambda_ = 0.1\n",
    "w, loss = ridge_regression_s(y_s, x_poly, lambda_)\n",
    "x_poly[:1]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> b64f4377a2e22340c55906c3b8b59d4bb046fcf2
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT RUN\n",
    "indices = build_k_indices(y_s, 4, seed=1)\n",
    "x_test = x_s[indices[0]]\n",
    "x_train = np.delete(x_s, [indices[0]], axis=0)\n",
    "x_train.shape\n",
    "loss_tr, loss_te = cross_validation_rr(y_s, x_s, k_indices=indices, k=1, lambda_=0.1, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=4\n",
    "def cross_validation_demo_RR(x,y,K):\n",
    "    seed = 1\n",
    "    degree = 4\n",
    "    k_fold = K\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    x_test = x[k_indices[0]]\n",
    "    x_train = np.delete(x, [indices[0]], axis=0)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for i in range(len(lambdas)):\n",
    "        l = lambdas[i]\n",
    "        avg_err_tr = 0\n",
    "        avg_err_te = 0\n",
    "        for k in range(k_fold):\n",
    "            err = cross_validation_rr(y, x, k_indices, k, l, degree)\n",
    "            avg_err_tr += err[0]\n",
    "            avg_err_te += err[1]\n",
    "        rmse_tr.append(np.sqrt(2 * avg_err_tr / k_fold))\n",
    "        rmse_te.append(np.sqrt(2 * avg_err_te / k_fold))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    min_err_index = 0\n",
    "    for i in range(1, len(rmse_te)):\n",
    "        if rmse_te[i] < rmse_te[min_err_index]:\n",
    "            min_err_index = i\n",
    "            \n",
    "    print('Best lambda is: {0}'.format(lambdas[min_err_index]))       \n",
    "    \n",
<<<<<<< HEAD
    "cross_validation_demo_RR(x_s,y_s,4)\n",
=======
    "cross_validation_demo_RR(x_inorder,y,K)\n",
>>>>>>> b64f4377a2e22340c55906c3b8b59d4bb046fcf2
    "\n",
    "degree_opt = 4\n",
    "lambda_opt = 0.0006723357536499335\n",
    "x_poly = build_poly(x_s, degree_opt)\n",
    "w_rr_opt, loss_tr = ridge_regression_s(y_s, x_poly, lambda_opt)\n",
    "print(\"Training set mse: {}\".format(loss_tr))\n",
    "\n",
    "#Training Accuracy\n",
    "y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "accuracy = []\n",
    "accuracy.append((list(y_clean == y_predicted).count(True))/len(y_clean))\n",
    "print(\"accuracy = {val}\".format(val=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_factors(x):\n",
    "    # This function takes a number and prints the factors\n",
    "    a = []\n",
    "    for i in range(2,10):\n",
    "        if x % i == 0:\n",
    "            a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_LS=tX_clean.copy()\n",
    "#y_LS= y_clean.copy()\n",
    "#accuracy=0.7239627683002026 mse=0.7401139564470155\n",
    "\n",
    "#tX_LS=tX_nan.copy()\n",
    "#y_LS= y.copy()\n",
    "# accuracy=0.745088 mse=0.6786542286532609\n",
    "\n",
    "def compute_least_squares(x, y):\n",
    "    x_LS=x.copy()\n",
    "    y_LS= y.copy()\n",
    "\n",
    "    K_values = return_factors(len(x_LS))\n",
    "    accuracy = []\n",
    "    #K-fold crossvalidation\n",
    "    for K in K_values:\n",
    "        #Initialization\n",
    "        list_x_LS = np.split(x_LS,K)\n",
    "        list_y_LS = np.split(y_LS,K)\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        opt_w = []\n",
    "        for ind, x_bloc in enumerate(list_x_LS):\n",
    "            x_test = x_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            x_train = np.concatenate(list_x_LS[:ind] + list_x_LS[ind+1:])\n",
    "            y_train = np.concatenate(list_y_LS[:ind] + list_y_LS[ind+1:])\n",
    "            mse_LS, optimal_weights_LS = least_squares(y_train,x_train)\n",
    "            mse_errors.append(compute_mse(y_test, x_test, optimal_weights_LS))\n",
    "            weights.append(optimal_weights_LS)\n",
    "\n",
    "        opt_w = weights[np.argmin(mse_errors)]\n",
    "        y_model = predict_labels(opt_w, x_LS)\n",
    "\n",
    "        #Computing accuracy\n",
    "        accuracy.append((list(y_model == y_LS).count(True))/len(y_model))\n",
    "        print(\"accuracy = {val} mse={mse}\".format(mse = mse_LS, val=accuracy[-1]))\n",
    "\n",
    "    #Plot of accuracies\n",
    "    print(\"\\nMaximum accuracy = {val}\".format(val=np.max(accuracy)))\n",
    "    #plt.plot(K_values, accuracy, '.-', markersize=15, label = \"Accuracy\");\n",
    "    #plt.xlabel(\"K value\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Accuracies for Least Squares\")\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.339992 mse=0.16916771391528657\n",
      "accuracy = 0.339944 mse=0.1693944515525672\n",
      "accuracy = 0.339952 mse=0.16935851609831304\n",
      "accuracy = 0.339924 mse=0.16936699783814213\n",
      "\n",
      "Maximum accuracy = 0.339992\n"
     ]
    }
   ],
   "source": [
    "compute_least_squares(x_inorder, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Least squares gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can alter on gamma + w_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LS[ind]\n",
    "            tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "        \n",
    "            mse, opt_w = least_squares_GD(y_train, tX_train, w_initial, max_iters, gamma)\n",
    "            mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\" gamma={l:.3f},mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD, tX_LS)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LS == y_predicted).count(True))/len(y_LS))\n",
    "    print(accuracy)\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "print(\"dataset1\")\n",
    "x_LS, m_X,s = standardize(x_clean.copy())\n",
    "x_LS = x_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "cross_validation_LS_GD_demo(x_LS,y_LS,5)\n",
    "\n",
    "#With nan value replaced\n",
    "print(\"dataset2\")\n",
    "x_LS=x_nan.copy()\n",
    "y_LS=y.copy()\n",
    "cross_validation_LS_GD_demo(x_LS,y_LS,5)\n",
    "#With x_te\n",
    "print(\"dataset3\")\n",
    "x_LS =x_s\n",
    "y_LS=y_s\n",
    "cross_validation_LS_GD_demo(x_LS,y_LS,4)\n",
    "#Data subgoup\n",
    "x_LS = x_inorder\n",
    "y_LS =y.copy()\n",
    "cross_validation_LS_GD_demo(x_LS,y_LS,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data subgoup\n",
    "x_LS = x_inorder\n",
    "y_LS =y.copy()\n",
    "cross_validation_LS_GD_demo(x_LS,y_LS,5)"
   ]
  },
  {
>>>>>>> b64f4377a2e22340c55906c3b8b59d4bb046fcf2
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the gamma and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    max_iters = 50\n",
    "    max_batch_size = 32\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    list_tX_LS = np.split(tX_LS,K)\n",
    "    list_y_LS = np.split(y_LS,K)\n",
    "\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind_batch,batch_size in enumerate(batch_sizes):  \n",
    "        result_mse_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LS):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LS[ind]\n",
    "                tX_train= list_tX_LS[:ind] + list_tX_LS[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LS[:ind] + list_y_LS[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                sgd_mse, opt_w = least_squares_SGD(y_train, tX_train, w_initial, batch_size, max_iters, gamma)\n",
    "                mse_errors.append(compute_mse(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_mse_gamma.append(np.mean(mse_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_mse.append(np.min(result_mse_gamma))\n",
    "        result_gamma.append(gammas[np.argmin(result_mse_gamma)])\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_mse_gamma)])\n",
    "\n",
    "    print(\" gamma={l:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = np.nanmin(result_mse), l =result_gamma[np.nanargmin(result_mse)], b=np.nanargmin(result_mse)))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "    \n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD, tX_LS)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LS == y_predicted).count(True))/len(y_LS))\n",
    "    print(accuracy)\n",
    "\n",
    "#With tX_CLEAN : accuracy = , mse= , gamma = , batch=\n",
    "#With tX_NAN : accuracy= , mse= , gamma = ,batch=\n",
    "\n"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "print(\"dataset1\")\n",
    "x_LS, m_X,s = standardize(x_clean.copy())\n",
    "x_LS = x_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "#cross_validation_LS_SGD_demo(x_LS,y_LS,5)\n",
    "#With nan value replaced\n",
    "print(\"dataset2\")\n",
    "x_LS=x_nan.copy()\n",
    "y_LS=y.copy()\n",
    "cross_validation_LS_SGD_demo(x_LS,y_LS,5)\n",
    "#With x_te\n",
    "print(\"dataset3\")\n",
    "x_LS =x_s\n",
    "y_LS=y_s\n",
    "cross_validation_LS_SGD_demo(x_LS,y_LS,4)"
   ]
  },
  {
>>>>>>> b64f4377a2e22340c55906c3b8b59d4bb046fcf2
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (not regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LR.shape[1])\n",
    "\n",
    "    list_tX_LR = np.split(tX_LR,K)\n",
    "    list_y_LR = np.split(y_LR,K)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for ind, gamma in enumerate(gammas):\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        #K-fold crossvalidation\n",
    "        for ind, tX_bloc in enumerate(list_tX_LR):\n",
    "            tX_test = tX_bloc\n",
    "            y_test = list_y_LR[ind]\n",
    "            tX_train= list_tX_LR[:ind] + list_tX_LR[ind+1:]\n",
    "            tX_train= np.concatenate(tX_train)\n",
    "            y_train= list_y_LR[:ind] + list_y_LR[ind+1:]\n",
    "            y_train=np.concatenate(y_train)\n",
    "            loss, opt_w = logistic_regression(y_train,tX_train,w_initial, max_iters, gamma)\n",
    "            loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "            weights.append(opt_w)\n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "\n",
    "\n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\" gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR, tX_LR)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LR == y_predicted).count(True))/len(y_LR))\n",
    "    print(accuracy)\n",
    "\n",
    "#With tX_CLEAN : accuracy = 0.6823, loss= , gamma = \n",
    "#With tX_NAN : accuracy= 0.34, loss= nan, gamma = 0.114\n",
    "#With x_te_s : accuracy = 0.7015, loss = 16775, gamma = 0.144"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "print(\"dataset1\")\n",
    "x_LS, m_X,s = standardize(x_clean.copy())\n",
    "x_LS = x_LS[0:68110]\n",
    "y_LS = y_clean[0:68110].copy()\n",
    "cross_validation_LR_demo(x_LS,y_LS,5)\n",
    "\n",
    "#With nan value replaced\n",
    "print(\"dataset2\")\n",
    "x_LS=x_nan.copy()\n",
    "y_LS=y.copy()\n",
    "cross_validation_LR_demo(x_LS,y_LS,5)\n",
    "#With x_te\n",
    "print(\"dataset3\")\n",
    "x_LS =x_s\n",
    "y_LS=y_s\n",
    "cross_validation_LR_demo(x_LS,y_LS,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data subgoup\n",
    "x_LS = x_inorder\n",
    "y_LS =y.copy()\n",
    "cross_validation_LR_demo(x_LS,y_LS,5)"
   ]
  },
  {
>>>>>>> b64f4377a2e22340c55906c3b8b59d4bb046fcf2
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression regularized with lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "\n",
    "    max_iters = 50\n",
    "    lambdas = np.logspace(-4,0,10)\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    list_tX_LRR = np.split(tX_LRR,K)\n",
    "    list_y_LRR = np.split(y_LRR,K)\n",
    "\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    result_gamma=[]\n",
    "    for ind,lambda_ in enumerate(lambdas):  \n",
    "        result_loss_gamma = []\n",
    "        result_opt_w_gamma = []\n",
    "        for ind_gamma,gamma in enumerate(gammas):\n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            #K-fold crossvalidation\n",
    "            for ind, tX_bloc in enumerate(list_tX_LRR):\n",
    "                tX_test = tX_bloc\n",
    "                y_test = list_y_LRR[ind]\n",
    "                tX_train= list_tX_LRR[:ind] + list_tX_LRR[ind+1:]\n",
    "                tX_train= np.concatenate(tX_train)\n",
    "                y_train= list_y_LRR[:ind] + list_y_LRR[ind+1:]\n",
    "                y_train=np.concatenate(y_train)\n",
    "        \n",
    "                loss, opt_w = reg_logistic_regression(y_train,tX_train,lambda_,w_initial,max_iters,gamma)\n",
    "                loss_errors.append(calculate_loss_logistic_reg(y_test, tX_test,opt_w))\n",
    "                weights.append(opt_w)\n",
    "    \n",
    "            result_loss_gamma.append(np.mean(loss_errors))\n",
    "            result_opt_w_gamma.append(np.mean(weights,axis=0))\n",
    "        result_loss.append(np.min(result_loss_gamma))\n",
    "        result_gamma.append(np.argmin(result_loss_gamma))\n",
    "        result_opt_w.append(result_opt_w_gamma[np.argmin(result_loss_gamma)])\n",
    "\n",
    "    print(np.min(result_loss))\n",
    "    print(result_gamma[np.argmin(result_loss)])\n",
    "    print(np.argmin(result_loss))\n",
    "    print(\" gamma={l:.3f}, batch={b:.0f}, mse={mse:.3f}\".format(mse = np.min(result_loss), l =result_gamma[np.argmin(result_loss)], b=np.argmin(result_loss)))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "    print(optimal_weights_LRR)\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR, tX_LRR)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_LRR == y_predicted).count(True))/len(y_LRR))\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With tX and y_LS no corrupted\n",
    "print(\"dataset1\")\n",
    "x, m_X,s = standardize(x_clean.copy())\n",
    "x = x_LS[0:68110]\n",
    "y = y_clean[0:68110].copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,5)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,5)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With nan value replaced\n",
    "print(\"dataset2\")\n",
    "x=x_nan.copy()\n",
    "y=y.copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,5)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,5)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With x_te\n",
    "print(\"dataset3\")\n",
    "x =x_s.copy()\n",
    "y =y_s.copy()\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x,y,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x,y,4)\n",
    "print(\"Least-Square -SDG\")\n",
    "cross_validation_LS_SGD_demo(x,y,4)\n",
    "print(\"Least-Square -GD\")\n",
    "cross_validation_LS_GD_demo(x,y,4)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x,y,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
