{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project on Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "- We only use imported data without any modification\n",
    "- We then cleaned it by removing all samples containing at least one Nan value\n",
    "- We then partioned our dataset into 4 subgroups, depending on the value of feature PRI_jet_num. We then standardized each subgroup individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nan values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the Higgs dataset, we saw that there were a lot of missing values, corresponding to the -999.0 value. We thus decided to remove the samples that had at least one -999.0 outlier value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 72.7544 % of our training dataset.\n"
     ]
    }
   ],
   "source": [
    "selector = np.all(x != -999.0, axis=1)\n",
    "\n",
    "x_clean = x[selector]\n",
    "y_clean = y[selector]\n",
    "x_clean,_,_ = standardize(x_clean)\n",
    "print(\"We removed\", (1-x_clean.shape[0]/x.shape[0])*100, \"% of our training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that too many data samples are lost in x_clean. So it is not a good idea to remove these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values in dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we chose an alternative way to deal with the Nan values. We decided to replace each NaN value by the mean of the feature it is in. The mean was computed without taking into account the Nan values in the feature. This is a standardization concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_NAN(x):\n",
    "    x_nan = x.copy()\n",
    "    x_nan[x_nan==-999.0] = np.nan\n",
    "    return (standardize(x_nan))\n",
    "\n",
    "# All the Nan (corresponding to unknown values) were replaced by the mean value of the feature it is in.\n",
    "def replace_mean(x_nan):\n",
    "    means_cols = np.nanmean(x_nan, axis=1)\n",
    "    is_nan = np.isnan(x_nan)\n",
    "    for col in range(x_nan.shape[1]):\n",
    "        x_nan[is_nan[:, col], col] = means_cols[col]\n",
    "    return (x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan, mean_x_nan, std_x_nan = standardize_NAN(x)\n",
    "x_nan = replace_mean(x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the outliers of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assumed that the dataset can have some ouliers. So to deal with them we implemented some methods that can remove the datasamples where some percentile is trespassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_percentiles(tX, tX_clean, i, percentile):\n",
    "    arguments = []\n",
    "    a = np.percentile(tX_clean[:,i],percentile)\n",
    "    tX_perc = tX.copy()\n",
    "    arguments = np.argwhere(tX_perc[tX[:,i] > round(a, 2)])\n",
    "    return list(set(arguments[:,0]))\n",
    "\n",
    "def remove_rows_by_percentiles(tX,tX_clean):\n",
    "    args = []\n",
    "    for i in range(tX.shape[1]):\n",
    "        args= args+get_ind_percentiles(tX,tX_clean,i,99.97)\n",
    "    flat_list = [item for item in args]\n",
    "    mylist = list(set(flat_list))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = remove_rows_by_percentiles(x,x_clean)\n",
    "x_perc = np.delete(x, arg, axis=0)\n",
    "y_perc = np.delete(y, arg, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionning of dataset, based on PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing some data analysis, we saw that a specific column was only composed of 4 discrete values. This column was the \"PRI_jet_num\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names and their respective indices\n",
    "string_features = 'DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt'\n",
    "features = string_features.split(\",\")\n",
    "dict = {}\n",
    "for ind, feat in enumerate(features):\n",
    "    dict[feat] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus partitionned our dataset in the 4 different subgroups, corresponding to the value of the PRI_jet_num. This way, when doing the standardization on the dataset, we didn't bias our samples: indeed, the 4 groups have different kind of means and standard deviations values, which we used when standardizing each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subgrouping\n",
    "def subgrouping(x,ids,dict):\n",
    "    x_0=x[x[:,dict['PRI_jet_num']]==0]\n",
    "    x_1=x[x[:,dict['PRI_jet_num']]==1]\n",
    "    x_2=x[x[:,dict['PRI_jet_num']]==2]\n",
    "    x_3=x[x[:,dict['PRI_jet_num']]==3]\n",
    "    x_0 = np.delete(x_0,dict['PRI_jet_num'],1)\n",
    "    x_1 = np.delete(x_1,dict['PRI_jet_num'],1)\n",
    "    x_2 = np.delete(x_2,dict['PRI_jet_num'],1)\n",
    "    x_3 = np.delete(x_3,dict['PRI_jet_num'],1)\n",
    "    x_list = [x_0, x_1, x_2, x_3]\n",
    "\n",
    "    ids_0=ids[x[:,dict['PRI_jet_num']]==0]\n",
    "    ids_1=ids[x[:,dict['PRI_jet_num']]==1]\n",
    "    ids_2=ids[x[:,dict['PRI_jet_num']]==2]\n",
    "    ids_3=ids[x[:,dict['PRI_jet_num']]==3]\n",
    "    ids_list = [ids_0]\n",
    "    ids_list.append(ids_1)\n",
    "    ids_list.append(ids_2)\n",
    "    ids_list.append(ids_3)\n",
    "    \n",
    "\n",
    "    #Standardization of subgroups\n",
    "    mean = []\n",
    "    std = []\n",
    "    x_nan_replaced = []\n",
    "    for i in range(4):\n",
    "        x_arr,m,s = standardize_NAN(x_list[i])\n",
    "        print(i, m, s)\n",
    "        x_nan_replaced.append(replace_mean(x_arr))\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return x_nan_replaced, ids_list\n",
    "    \n",
    "#Grouping them back again\n",
    "def group(l,ids,dict):\n",
    "    ls = l.copy()\n",
    "    for i in range(4):\n",
    "        ls[i] = np.insert(ls[i],dict['PRI_jet_num'],np.ones((len(ids),1))*i,axis=1)\n",
    "    data_ord = np.insert(ls[0],0,ids[0], axis=1)\n",
    "    for i in range(1,4):\n",
    "        a = np.insert(ls[i],0,ids[i], axis=1)\n",
    "        data_ord = np.concatenate((data_ord, a))\n",
    "    x_new = data_ord[data_ord[:,0].argsort()]\n",
    "    x_new = x_new[:,1:]\n",
    "    \n",
    "    #print(f\"groups: tx {np.mean(x_new, axis=0)}{np.std(x_new,axis=0)}\")\n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmal\\OneDrive\\Documents\\Cours M1 S1\\Machine Learning\\ML_course-master\\ML_course\\projects\\project1\\scripts\\implementations.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm[:,valid_columns] =  ( x_valid_cols - mean_x[None, :] ) / std_x[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1.20667654e+02  5.87862388e+01  8.18703092e+01  1.38238670e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.66496128e+00\n",
      "  1.38238669e+01  7.63770107e+01  1.39276344e+00 -9.10076857e-01\n",
      "  0.00000000e+00  3.40127231e+01 -2.48576361e-02 -1.56573619e-02\n",
      "  4.23642875e+01 -5.23114410e-02  4.23519862e-02  3.15367606e+01\n",
      " -2.44434558e-02  1.25860810e+02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00] [51.74971089 32.003391   38.04347886 16.67462352  0.          0.\n",
      "  0.          0.69329161 16.67462348 23.56093879  0.5815929   0.93670201\n",
      "  0.         15.2294643   1.23342423  1.81734217 14.58590597  1.31084863\n",
      "  1.81783473 20.29443907  1.81099743 53.0863344   0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n",
      "1 [ 1.22182109e+02  4.60536000e+01  8.22190327e+01  6.59030903e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.33968637e+00\n",
      "  1.66449998e+01  1.50368035e+02  1.44418464e+00  2.35611975e-01\n",
      "  0.00000000e+00  3.85893680e+01 -1.73527288e-03 -7.31197772e-03\n",
      "  4.67665026e+01  6.68554627e-03  4.48938280e-02  4.01233713e+01\n",
      " -5.57938719e-03  2.03324833e+02  6.50121630e+01 -8.00435882e-04\n",
      " -1.50522284e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.50121632e+01] [59.34450047 35.58096046 42.28784944 47.43070109  0.          0.\n",
      "  0.          0.73762801 17.00781447 65.18166668  0.84641974  1.1063327\n",
      "  0.         21.00556477  1.21818737  1.81444814 21.33541491  1.26220634\n",
      "  1.8154439  28.35714127  1.81192526 81.90061285 45.15421873  1.82158106\n",
      "  1.81751683  0.          0.          0.         45.15421875]\n",
      "2 [ 1.22653136e+02  3.83436116e+01  7.92133481e+01  1.02985028e+02\n",
      "  2.60653431e+00  3.91406450e+02 -1.11545755e+00  2.06060702e+00\n",
      "  1.72806027e+01  2.45776275e+02  1.45346067e+00  5.65612815e-01\n",
      "  4.92867504e-01  4.46522669e+01 -7.75858989e-04  6.93356359e-03\n",
      "  5.09439375e+01 -3.46678179e-03  4.68206197e-02  5.37573689e+01\n",
      "  3.34000278e-03  2.96011170e+02  9.84702076e+01 -2.44786518e-03\n",
      " -1.32897040e-02  5.17098657e+01 -1.36306794e-02  6.14402827e-03\n",
      "  1.50180076e+02] [5.57490423e+01 3.52735935e+01 3.99167316e+01 7.05285845e+01\n",
      " 1.81260349e+00 4.26848181e+02 3.82103428e+00 7.59346693e-01\n",
      " 2.04094791e+01 1.01355960e+02 1.03438579e+00 9.78237122e-01\n",
      " 3.98565482e-01 2.79837976e+01 1.17217880e+00 1.81582136e+00\n",
      " 2.68817529e+01 1.19361204e+00 1.81531403e+00 4.02302623e+01\n",
      " 1.81482674e+00 1.11482689e+02 6.18663652e+01 1.81413389e+00\n",
      " 1.80990025e+00 2.54598156e+01 2.13480147e+00 1.81784901e+00\n",
      " 7.76454786e+01]\n",
      "3 [ 1.23189990e+02  4.21202026e+01  7.89255219e+01  1.26066343e+02\n",
      "  1.94277017e+00  3.27179877e+02 -1.53947347e-01  1.88462029e+00\n",
      "  5.35485356e+01  3.58008310e+02  1.58073610e+00  5.45345109e-01\n",
      "  3.79694324e-01  4.67709378e+01 -3.88079769e-03 -1.17623173e-02\n",
      "  5.59169127e+01  2.68678939e-04  3.67354268e-02  6.58187639e+01\n",
      "  7.97703483e-03  4.14852725e+02  1.23107808e+02 -1.38099170e-02\n",
      " -1.04990074e-03  7.12484564e+01 -7.78699693e-03 -1.91442880e-02\n",
      "  2.55320458e+02] [ 70.89775021  38.07417941  48.78977838  83.81996789   1.46989267\n",
      " 317.27268684   2.86726977   0.8272412   32.29848326 150.13499581\n",
      "   1.24713314   0.94333732   0.38762705  32.43010842   1.20470959\n",
      "   1.8241134   32.96582687   1.21765619   1.81806005  49.69728866\n",
      "   1.81234158 158.82544995  76.31203429   1.57322807   1.80673322\n",
      "  40.12542351   1.77531376   1.8147406  128.45133644]\n"
     ]
    }
   ],
   "source": [
    "x_subgroups_list, ids_list = subgrouping(x,ids,dict)\n",
    "x_subgroups = group(x_subgroups_list, ids_list,dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying all features that had Nan Values, we saw that many sample of our dataset had to replace their Nan value ba the mean value of the feature, as explained above in the standardization part. The problem is that having a mean value for 75% of our datasample is bad for some features. That's why we decided to remove some of them, and play only with the features that are essential to our model.\n",
    "\n",
    "Indeed, the columns that were removed were all features that derived from some primitive ones. And as most of them had many Nan values, we assumed it was a good idea to remove them and see how our model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns0 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "selected_columns1 = [1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]\n",
    "selected_columns_ideal = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]\n",
    "\n",
    "def selected_non_nan_columns(x):\n",
    "    x_selected = np.zeros((len(x), len(selected_columns0)))\n",
    "    for i in range(len(x)):\n",
    "        s = np.take(x[i], indices=selected_columns0, axis=0)\n",
    "        x_selected[i] = s\n",
    "    return x_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = selected_non_nan_columns(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build polynomial features, meaning we expanded the number of features we had by adding features with an incremeneted degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    nb_features = x.shape[1]\n",
    "    nb_samples = x.shape[0]\n",
    "    x_poly = np.ones((nb_samples, 1))\n",
    "    for d in range(1, degree+1):\n",
    "        x_d = x**d\n",
    "        x_poly = np.hstack((x_poly, x_d))\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_s, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of the different ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least Squares Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_GD_demo(x_LS,y_LS,K): \n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "    seed=1\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_mse =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for gamma in gammas:\n",
    "        weights=[]\n",
    "        mse_errors = []\n",
    "        for k in range(K):\n",
    "            mse_te, opt_w = cross_validation_ls_GD(y_LS, tX_LS,k_indices,k, gamma,max_iters,w_initial)\n",
    "            mse_errors.append(mse_te)\n",
    "            weights.append([opt_w])\n",
    "        \n",
    "        gen_mse.append(np.mean(mse_errors))\n",
    "        gen_opt_w.append(np.mean(weights, axis=0))\n",
    "        \n",
    "    del weights\n",
    "    del mse_errors\n",
    "    optimal_gamma_LS_GD = gammas[np.nanargmin(gen_mse)]\n",
    "    optimal_weights_LS_GD = gen_opt_w[np.nanargmin(gen_mse)]\n",
    "    mse_LS_GD = np.nanmin(gen_mse)\n",
    "    print(\"   gamma={l:.3f}, mse={mse:.3f}\".format(mse = mse_LS_GD, l = optimal_gamma_LS_GD))\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_GD.T, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted.flatten()).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    #return accuracy,optimal_gamma_LS_GD, optimal_wights_LS_GD,mse_LS_GD\n",
    "    \n",
    "    del gen_opt_w\n",
    "    del gen_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least Squares Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LS_SGD_demo(x_LS,y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "\n",
    "    seed=1\n",
    "    max_iters = 50\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    batch_sizes = np.array([2,4,6,8])\n",
    "    \n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "\n",
    "    result_mse =[]\n",
    "    result_opt_w=[]\n",
    "    \n",
    "    hyperparams = [(batch_size,gamma) for batch_size in batch_sizes for gamma in gammas ]\n",
    "    \n",
    "    \n",
    "    for batch_size,gamma in hyperparams:  \n",
    "            mse_errors=[]\n",
    "            weights=[]\n",
    "            \n",
    "            for k in range(K):\n",
    "                mse_te, opt_w = cross_validation_ls_SGD(y_LS, tX_LS, k_indices, k, gamma, max_iters, w_initial,batch_size)\n",
    "                mse_errors.append(mse_te)\n",
    "                weights.append([opt_w])\n",
    "    \n",
    "            result_mse.append(np.mean(mse_errors))\n",
    "            result_opt_w.append(np.mean(weights,axis=0))\n",
    "            \n",
    "    del mse_errors\n",
    "    del weights\n",
    "    \n",
    "    mse = np.min(result_mse)\n",
    "    hyper_opt= hyperparams[np.argmin(result_mse)]\n",
    "    print(\"   gamma={g:.3f}, batch={b:.2f}, mse={mse:.3f}\".format(mse = mse, g=hyper_opt[1], b=hyper_opt[0]))\n",
    "\n",
    "    optimal_weights_LS_SGD = result_opt_w[np.nanargmin(result_mse)]\n",
    "\n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LS_SGD.T, tX_LS)\n",
    "    accuracy = (list(y_LS == y_predicted.flatten()).count(True))/len(y_LS)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del result_mse\n",
    "    del result_opt_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_squares(x_LS, y_LS,K):\n",
    "    #Adding constant term\n",
    "    tX_LS = np.c_[np.ones((y_LS.shape[0], 1)), x_LS]\n",
    "    \n",
    "    seed = 1\n",
    "    weights=[]\n",
    "    mse_errors = []\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(tX_LS.shape[1])\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LS, K, seed)\n",
    "    \n",
    "    for k in range(K):\n",
    "            mse_te, opt_w = cross_validation_ls(y_LS, tX_LS,k_indices,k)\n",
    "            mse_errors.append(mse_te)\n",
    "            weights.append([opt_w])\n",
    "    \n",
    "    mse_LS = np.min(mse_errors)\n",
    "    opt_w = weights[np.argmin(mse_errors)]\n",
    "    y_model = predict_labels(np.array(opt_w).T, tX_LS)\n",
    "\n",
    "    #Computing accuracy\n",
    "    print(\"   mse={mse}\".format(mse = mse_LS))\n",
    "    accuracy = (list(y_model.flatten() == y_LS).count(True))/len(y_model)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del mse_errors\n",
    "    del weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo_RR(x,y,K=4):\n",
    "    # Investigate the rmse loss with ridge regression for a given degree and different lambdas\n",
    "    # Print the best lambda, according to the rmse loss of the testing set (using cross-validation)\n",
    "    degree_opt = 4\n",
    "    lambda_opt= ridge_regression_invest(degree_opt, y, x)\n",
    "    \n",
    "    x_poly = build_poly(x_clean, degree_opt)\n",
    "    w_rr_opt, loss_tr = ridge_regression(y_clean, x_poly, lambda_opt)\n",
    "    print(\"Training set mse: {}\".format(loss_tr))\n",
    "    \n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(w_rr_opt, x_poly)\n",
    "    accuracy = []\n",
    "    accuracy.append((list(y_clean == y_predicted).count(True))/len(y_clean))\n",
    "    print(\"accuracy = {val}\".format(val=accuracy))\n",
    "    \n",
    "    del w_rr_opt\n",
    "    del x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LR_demo(x_LR,y_LR,K):\n",
    "    #Adding constant term\n",
    "    tX_LR = np.c_[np.ones((y_LR.shape[0], 1)), x_LR]\n",
    "    max_iters = 100\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    seed=1\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LR, K, seed)\n",
    "\n",
    "    gen_opt_w=[]\n",
    "    gen_loss =[]\n",
    "\n",
    "    #gamma selection\n",
    "    for gamma in gammas:\n",
    "        weights=[]\n",
    "        loss_errors = []\n",
    "        \n",
    "        for k in range(K):\n",
    "            loss_te, opt_w = cross_validation_lr(y_LR, tX_LR, k_indices, k, gamma, max_iters, w_initial)\n",
    "            loss_errors.append(loss_te)\n",
    "            weights.append([opt_w])\n",
    "    \n",
    "        gen_loss.append(np.mean(loss_errors))\n",
    "        gen_opt_w.append(np.mean(weights,axis=0))\n",
    "    \n",
    "    del weights\n",
    "    del loss_errors\n",
    "        \n",
    "    optimal_gamma_LR = gammas[np.nanargmin(gen_loss)]\n",
    "    optimal_weights_LR = gen_opt_w[np.nanargmin(gen_loss)]\n",
    "    print(\"   gamma={l:.3f},loss={loss:.3f}\".format(loss = np.min(gen_loss), l = optimal_gamma_LR))\n",
    "\n",
    "     #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LR.T, tX_LR)\n",
    "    accuracy = (list(y_predicted.flatten() == y_LR).count(True))/len(y_LR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del gen_opt_w\n",
    "    del gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_LRR_demo(x_LRR,y_LRR,K):\n",
    "    #Adding constant term\n",
    "    tX_LRR = np.c_[np.ones((y_LRR.shape[0], 1)), x_LRR]\n",
    "\n",
    "    seed = 1\n",
    "    max_iters = 50\n",
    "    lambdas = np.logspace(-4,0,10)\n",
    "    gammas = np.logspace(-4,0,20)\n",
    "    hyperparams = [(gamma,lambda_) for gamma in gammas for lambda_ in lambdas]\n",
    "\n",
    "    w_initial = np.zeros(tX_LRR.shape[1])\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_LRR, K, seed)\n",
    "\n",
    "    result_loss =[]\n",
    "    result_opt_w=[]\n",
    "    for gamma,lambda_ in hyperparams:  \n",
    "            loss_errors=[]\n",
    "            weights=[]\n",
    "            \n",
    "            for k in range(K):\n",
    "                loss_te, opt_w = cross_validation_lrr(y_LRR, tX_LRR, k_indices, k, lambda_, gamma, max_iters, w_initial)\n",
    "                loss_errors.append(loss_te)\n",
    "                weights.append([opt_w])\n",
    "    \n",
    "            result_loss.append(np.mean(loss_errors))\n",
    "            result_opt_w.append(np.mean(weights,axis=0))\n",
    "\n",
    "    \n",
    "    del loss_errors\n",
    "    del weights\n",
    "    mse = np.min(result_loss)\n",
    "    hyper_opt= hyperparams[np.argmin(result_loss)]\n",
    "    print(\"   gamma={g:.3f}, mse={mse:.3f} lambda{l:.3f}\".format(mse = mse, g=hyper_opt[0], l=hyper_opt[1]))\n",
    "\n",
    "    optimal_weights_LRR = result_opt_w[np.argmin(result_loss)]\n",
    "   \n",
    "    #Training Accuracy\n",
    "    y_predicted = predict_labels(optimal_weights_LRR.T, tX_LRR)\n",
    "    accuracy = (list(y_predicted.flatten() == y_LRR).count(True))/len(y_LRR)\n",
    "    print(\"   accuracy={acc:.3f}\".format(acc=accuracy))\n",
    "    \n",
    "    del result_loss\n",
    "    del result_opt_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for cleaned dataset (with removed samples containing Nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-Square-GD\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b4b40b8da670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m68110\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-GD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcross_validation_LS_GD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least-Square-SGD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcross_validation_LS_SGD_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-77e63d2fc81a>\u001b[0m in \u001b[0;36mcross_validation_LS_GD_demo\u001b[1;34m(x_LS, y_LS, K)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmse_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mmse_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_ls_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_LS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mmse_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Cours M1 S1\\Machine Learning\\ML_course-master\\ML_course\\projects\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_ls_GD\u001b[1;34m(y, x, k_indices, k, gamma, max_iters, w_initial)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mmse_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmse_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Cours M1 S1\\Machine Learning\\ML_course-master\\ML_course\\projects\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Cours M1 S1\\Machine Learning\\ML_course-master\\ML_course\\projects\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;34m\"\"\"Compute the gradient.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# w is (n_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# e shape is n_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x1, m_X, s = standardize(x_clean.copy())\n",
    "x1 = x1[0:68110]\n",
    "y1 = y_clean[0:68110].copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x1,y1,5)\n",
    "print(\"Least-Square-SGD\")\n",
    "cross_validation_LS_SGD_demo(x1,y1,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x1,y1,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x1,y1,K=5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b982f6b1f4a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcross_validation_LR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularized Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcross_validation_LRR_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-d87600a909c4>\u001b[0m in \u001b[0;36mcross_validation_LR_demo\u001b[1;34m(x_LR, y_LR, K)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# split data in k fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_k_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_LR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgen_opt_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x1,y1,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x1,y1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= x_nan.copy()\n",
    "y2= y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x2,y2,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x2,y2,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x2,y2,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x2,y2,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x2,y2,5)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x2,y2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for standardized subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x3 = x_subgroups\n",
    "y3 = y.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x3,y3,5)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x3,y3,5)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x3,y3,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x3,y3,5)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x3,y3,2)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x3,y3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 =x_s.copy()\n",
    "y4 =y_s.copy()\n",
    "print(\"Least-Square-GD\")\n",
    "cross_validation_LS_GD_demo(x4,y4,4)\n",
    "print(\"Least-Square-SDG\")\n",
    "cross_validation_LS_SGD_demo(x4,y4,4)\n",
    "print(\"Least-Square\")\n",
    "compute_least_squares(x4,y4,5)\n",
    "print(\"Ridge Regression\")\n",
    "cross_validation_demo_RR(x4,y4,4)\n",
    "print(\"Logistic Regression\")\n",
    "cross_validation_LR_demo(x4,y4,4)\n",
    "print(\"Regularized Logistic Regression\")\n",
    "cross_validation_LRR_demo(x4,y4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and Save output in CSV format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './logisticRegression_x_te_s' # TODO: fill in desired name of output file for submission\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]\n",
    "tX_test = selected_non_nan_columns(tX_test)\n",
    "y_pred = predict_labels(optimal_weights_LR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
